<!doctype html><html lang="en" class="scroll-smooth"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning</title><link rel="canonical" href="https://dqd-rl.github.io/"><link rel="alternate" type="application/atom+xml" title="" href="/feed.xml"><meta name="description" content="Official website for the paper 'Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning'"><meta property="og:title" content="Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning"><meta property="og:type" content="website"><meta property="og:url" content="https://dqd-rl.github.io/"><meta property="og:image" content="https://dqd-rl.github.io/static/open-graph-preview.png"><meta property="og:description" content="Official website for the paper 'Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning'"><meta property="og:image:alt" content="Diagram of our proposed algorithms, CMA-MEGA (ES) and CMA-MEGA (TD3, ES)."><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning"><meta name="twitter:description" content="Official website for the paper 'Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning'"><meta name="twitter:site" content="btjanaka"><meta name="twitter:creator" content="btjanaka"><meta name="twitter:image" content="https://dqd-rl.github.io/static/twitter-preview.png"><meta name="twitter:image:alt" content="Diagram of our proposed algorithms, CMA-MEGA (ES) and CMA-MEGA (TD3, ES)."><link rel="stylesheet" href="/assets/main.css?8ba9cc72295f3792a4fb556a1480daf6"><link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.1/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"><link rel="icon" href="/favicon.ico" sizes="any"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><script>var _paq = window._paq = window._paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://dqd-rl.matomo.cloud/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src='//cdn.matomo.cloud/dqd-rl.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
  })();</script></head><body class="flex text-gray-800 dark:bg-gray-900 dark:selection:bg-gray-700 dark:text-darktext flex-col font-serif min-h-screen selection:bg-[rgba(192,192,192,0.5)]"><main class="w-full mx-auto"><figure class="w-full"><div class="w-full grid grid-cols-2 sm:grid-cols-3"><div class="w-full relative col-span-2 sm:col-span-1"><video src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" type="video/mp4" class="w-full b-lazy" autoplay="" muted="" playsinline="" loop="" preload="auto" data-src="static/cma-mega-es-qd-half-cheetah-best.mp4"></video><table class="text-sm absolute bottom-0 font-bold left-0 mb-1 md:text-2xl ml-1 sm:mb-2 sm:ml-2 sm:text-xl text-black"><tbody><tr><td class="pr-1 sm:pr-3">Front Foot:</td><td class="text-right">38.6%</td></tr><tr><td class="pr-1 sm:pr-3">Back Foot:</td><td class="text-right">57.2%</td></tr></tbody></table></div><div class="w-full relative"><video src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" type="video/mp4" class="w-full b-lazy" autoplay="" muted="" playsinline="" loop="" preload="auto" data-src="static/cma-mega-es-qd-half-cheetah-back-foot.mp4"></video><table class="text-sm absolute bottom-0 font-bold left-0 mb-1 md:text-2xl ml-1 sm:mb-2 sm:ml-2 sm:text-xl text-black"><tbody><tr><td class="pr-1 sm:pr-3">Front Foot:</td><td class="text-right">5.8%</td></tr><tr><td class="pr-1 sm:pr-3">Back Foot:</td><td class="text-right">96.8%</td></tr></tbody></table></div><div class="w-full relative"><video src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" type="video/mp4" class="w-full b-lazy" autoplay="" muted="" playsinline="" loop="" preload="auto" data-src="static/cma-mega-es-qd-half-cheetah-front-foot.mp4"></video><table class="text-sm absolute bottom-0 font-bold left-0 mb-1 md:text-2xl ml-1 sm:mb-2 sm:ml-2 sm:text-xl text-black"><tbody><tr><td class="pr-1 sm:pr-3">Front Foot:</td><td class="text-right">83.9%</td></tr><tr><td class="pr-1 sm:pr-3">Back Foot:</td><td class="text-right">11.1%</td></tr></tbody></table></div></div><div class="mx-auto max-w-screen-md px-2"><figcaption class="italic mt-2 text-justify">Three different walking behaviors for a half-cheetah agent. Each behavior differs in how often each of the half-cheetah's feet contacts the ground. All behaviors were generated by one run of a quality diversity algorithm. In our work, we develop two new quality diversity algorithms and compare them with existing methods.</figcaption></div></figure><div class="w-full mx-auto pt-20"><h1 class="ltx_title ltx_title_document">Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning<div class="w-full flex flex-wrap justify-between max-w-screen-md mx-auto my-3"><div class="w-full sm:w-14/60"><a href="https://github.com/icaros-usc/dqd-rl" title="Code" class="w-full rounded-lg block dark:hover:bg-gray-700 dark:text-gray-100 flex hover:bg-gray-200 items-center my-0 p-1.5 text-gray-800 transition"><p class="text-right mb-0 pr-2 sm:text-3xl sm:text-center text-2xl w-1/3" style="color:#000"><i class="fab fa-github"></i></p><p class="text-sm pl-2 text-left w-2/3">Code</p></a></div><div class="w-full sm:w-14/60"><a href="https://arxiv.org/abs/2202.03666" title="arXiv" class="w-full rounded-lg block dark:hover:bg-gray-700 dark:text-gray-100 flex hover:bg-gray-200 items-center my-0 p-1.5 text-gray-800 transition"><p class="text-right mb-0 pr-2 sm:text-3xl sm:text-center text-2xl w-1/3" style="color:green"><i class="ai ai-arxiv"></i></p><p class="text-sm pl-2 text-left w-2/3">arXiv</p></a></div><div class="w-full sm:w-14/60"><a href="https://www.dropbox.com/sh/adjj2w0mgphm9nu/AADXL4C8zyRNlS60STb62jGta?dl=0" title="Supplemental Material" class="w-full rounded-lg block dark:hover:bg-gray-700 dark:text-gray-100 flex hover:bg-gray-200 items-center my-0 p-1.5 text-gray-800 transition"><p class="text-right mb-0 pr-2 sm:text-3xl sm:text-center text-2xl w-1/3" style="color:#0061ff"><i class="fab fa-dropbox"></i></p><p class="text-sm pl-2 text-left w-2/3">Supplemental Material</p></a></div><div class="w-full sm:w-14/60"><a href="https://github.com/dqd-rl/dqd-rl.github.io/discussions" title="Discussion" class="w-full rounded-lg block dark:hover:bg-gray-700 dark:text-gray-100 flex hover:bg-gray-200 items-center my-0 p-1.5 text-gray-800 transition"><p class="text-right mb-0 pr-2 sm:text-3xl sm:text-center text-2xl w-1/3" style="color:#fcd43b"><i class="fa-comments fas"></i></p><p class="text-sm pl-2 text-left w-2/3">Discussion</p></a></div></div></h1><article class="ltx_document ltx_authors_1line"><div class="ltx_authors"><p class="ltx_creator ltx_role_author"><span class="ltx_personname"><a href="https://btjanaka.net" title="" class="ltx_ref ltx_href">Bryon Tjanaka</a><br class="ltx_break">University of Southern California<br class="ltx_break"><a href="mailto:tjanaka@usc.edu" title="" class="ltx_ref ltx_href ltx2_author_email">tjanaka@usc.edu</a></span></p><p class="ltx_creator ltx_role_author"><span class="ltx_personname"><a href="https://scholar.google.com/citations?user=RqSvzikAAAAJ" title="" class="ltx_ref ltx_href">Matthew C. Fontaine</a><br class="ltx_break">University of Southern California<br class="ltx_break"><a href="mailto:mfontain@usc.edu" title="" class="ltx_ref ltx_href ltx2_author_email">mfontain@usc.edu</a></span></p><p class="ltx_creator ltx_role_author"><span class="ltx_personname"><a href="http://julian.togelius.com" title="" class="ltx_ref ltx_href">Julian Togelius</a><br class="ltx_break">New York University<br class="ltx_break"><a href="mailto:julian@togelius.com" title="" class="ltx_ref ltx_href ltx2_author_email">julian@togelius.com</a></span></p><p class="ltx_creator ltx_role_author"><span class="ltx_personname"><a href="https://stefanosnikolaidis.net" title="" class="ltx_ref ltx_href">Stefanos Nikolaidis</a><br class="ltx_break">University of Southern California<br class="ltx_break"><a href="mailto:nikolaid@usc.edu" title="" class="ltx_ref ltx_href ltx2_author_email">nikolaid@usc.edu</a></span></p></div><div class="ltx_abstract"><h2 class="ltx_title ltx_title_abstract">Abstract</h2><p class="ltx_p">Consider a walking agent that must adapt to damage. To approach this task, we can train a collection of policies and have the agent select a suitable policy when damaged. Training this collection may be viewed as a quality diversity (QD) optimization problem, where we search for solutions (policies) which maximize an objective (walking forward) while spanning a set of measures (measurable characteristics). Recent work shows that differentiable quality diversity (DQD) algorithms greatly accelerate QD optimization when exact gradients are available for the objective and measures. However, such gradients are typically unavailable in RL settings due to non-differentiable environments. To apply DQD in RL settings, we propose to approximate objective and measure gradients with evolution strategies and actor-critic methods. We develop two variants of the DQD algorithm CMA-MEGA, each with different gradient approximations, and evaluate them on four simulated walking tasks. One variant achieves comparable performance (QD score) with the state-of-the-art PGA-MAP-Elites in two tasks. The other variant performs comparably in all tasks but is less efficient than PGA-MAP-Elites in two tasks. These results provide insight into the limitations of CMA-MEGA in domains that require rigorous optimization of the objective and where exact gradients are unavailable.</p></div><section id="S1" class="ltx_section"><h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><figure id="S1.F1" class="mx-auto max-w-screen-md ltx_figure mount_diagram"><img class="w-full b-lazy sm:w-3/4" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="Diagram of CMA-MEGA (ES) and CMA-MEGA (TD3, ES). Refer to caption." data-src="static/diagram.svg"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>CMA-MEGA (ES) and CMA-MEGA (TD3, ES). Each algorithm first approximates gradients around a mean solution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.967476em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span>. Then, each algorithm generates new solutions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\phi}}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93858em;vertical-align:-.24414em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.21752399999999997em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span>, evaluates them in the environment, and inserts them into an archive. Finally, internal parameters such as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.967476em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span> are updated based on how each <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\phi}}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93858em;vertical-align:-.24414em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.21752399999999997em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span> improves the archive. CMA-MEGA (TD3, ES) also trains TD3 from environment experience.</figcaption></figure><div id="S1.p1" class="ltx_para"><p class="ltx_p">We focus on the problem of extending differentiable quality diversity (DQD) to reinforcement learning (RL) domains. We propose to approximate gradients for the objective and measure functions, resulting in two variants of the DQD algorithm CMA-MEGA.</p></div><div id="S1.p2" class="ltx_para"><p class="ltx_p">Consider a half-cheetah agent (Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) trained for locomotion, where the agent must continue walking forward even when one foot is damaged. If we frame this challenge as an RL problem, we must design a reward function that results in a single robustly capable agent. However, prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="Deep reinforcement learning doesn’t work yet" class="ltx_ref">23</a>, <a href="#bib.bib54" title="Faulty reward functions in the wild" class="ltx_ref">7</a>]</cite> suggests that designing such a reward function is difficult.</p></div><div id="S1.p3" class="ltx_para"><p class="ltx_p">As an alternative approach, consider that we have intuition on what behaviors would be useful for adapting to damage. For instance, we can <span class="ltx_text ltx_font_italic">measure</span> how often each foot is used during training, and we can pre-train a collection of policies that are diverse in how the agent uses its feet. When one of the agent’s feet is damaged during deployment, the agent can adapt to the damage by selecting a policy that did not move the damaged foot during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="Robots that can adapt like animals" class="ltx_ref">12</a>, <a href="#bib.bib4" title="Scaling map-elites to deep neuroevolution" class="ltx_ref">8</a>]</cite>.</p></div><figure id="S1.F2" class="mx-auto max-w-screen-md ltx_figure mount_frames"><div class="w-full grid gap-3 grid-cols-1 sm:grid-cols-2"><div class="w-full rounded-lg overflow-hidden relative undefined"><video src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" type="video/mp4" class="w-full b-lazy" autoplay="" muted="" playsinline="" loop="" preload="auto" data-src="static/cma-mega-es-qd-half-cheetah-back-foot.mp4"></video><table class="text-sm absolute bottom-0 font-bold left-0 mb-1 md:text-2xl ml-1 sm:mb-2 sm:ml-2 sm:text-xl text-black"><tbody><tr><td class="pr-1 sm:pr-3 text-left">Front Foot:</td><td class="text-right">5.8%</td></tr><tr><td class="pr-1 sm:pr-3 text-left">Back Foot:</td><td class="text-right">96.8%</td></tr></tbody></table></div><div class="w-full rounded-lg overflow-hidden relative undefined"><video src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" type="video/mp4" class="w-full b-lazy" autoplay="" muted="" playsinline="" loop="" preload="auto" data-src="static/cma-mega-es-qd-half-cheetah-front-foot.mp4"></video><table class="text-sm absolute bottom-0 font-bold left-0 mb-1 md:text-2xl ml-1 sm:mb-2 sm:ml-2 sm:text-xl text-black"><tbody><tr><td class="pr-1 sm:pr-3 text-left">Front Foot:</td><td class="text-right">83.9%</td></tr><tr><td class="pr-1 sm:pr-3 text-left">Back Foot:</td><td class="text-right">11.1%</td></tr></tbody></table></div></div><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A half-cheetah agent executing two walking policies. On the left, the agent walks on its back foot while tapping the ground with its front foot. On the right, the agent walks on its front foot while jerking its back foot. Values on each video show the percentage of time each foot contacts the ground (each foot is measured individually, so values do not sum to 100%). With these policies, the agent could continue walking even if one foot is damaged.</figcaption></figure><div id="S1.p4" class="ltx_para"><p class="ltx_p">Pre-training such a collection of policies may be viewed as a quality diversity (QD) optimization problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="Quality diversity: a new frontier for evolutionary computation" class="ltx_ref">40</a>, <a href="#bib.bib7" title="Robots that can adapt like animals" class="ltx_ref">12</a>, <a href="#bib.bib8" title="Illuminating search spaces by mapping elites" class="ltx_ref">34</a>, <a href="#bib.bib4" title="Scaling map-elites to deep neuroevolution" class="ltx_ref">8</a>]</cite>. Formally, QD assumes an objective function <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> and one or more measure functions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi></mrow><annotation encoding="application/x-tex">{\bm{m}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span></span></span></span>. The goal of QD is to find solutions satisfying all output combinations of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi></mrow><annotation encoding="application/x-tex">{\bm{m}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span></span></span></span>, i.e. moving different combinations of feet, while maximizing each solution’s <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span>, i.e. walking forward quickly. Most QD algorithms treat <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi></mrow><annotation encoding="application/x-tex">{\bm{m}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span></span></span></span> as black boxes, but recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="Differentiable quality diversity" class="ltx_ref">16</a>]</cite> proposes differentiable quality diversity (DQD), which assumes <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi></mrow><annotation encoding="application/x-tex">{\bm{m}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span></span></span></span> are differentiable functions with exact gradient information.</p></div><div id="S1.p5" class="ltx_para"><p class="ltx_p">The recently proposed DQD algorithm CMA-MEGA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="Differentiable quality diversity" class="ltx_ref">16</a>]</cite> outperforms QD algorithms by orders of magnitude when exact gradients are available, such as when searching the latent space of a generative model. However, RL problems like the half-cheetah lack these gradients because the environment is typically non-differentiable, thus limiting the applicability of DQD. To address this limitation, we draw inspiration from how evolution strategies (ES) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="Bidirectional relation between cma evolution strategies and natural evolution strategies" class="ltx_ref">1</a>, <a href="#bib.bib20" title="Natural evolution strategies" class="ltx_ref">49</a>, <a href="#bib.bib9" title="Evolution strategies as a scalable alternative to reinforcement learning" class="ltx_ref">41</a>, <a href="#bib.bib23" title="Simple random search of static linear policies is competitive for reinforcement learning" class="ltx_ref">33</a>]</cite> and deep RL actor-critic methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="Trust region policy optimization" class="ltx_ref">43</a>, <a href="#bib.bib14" title="Proximal policy optimization algorithms" class="ltx_ref">44</a>, <a href="#bib.bib16" title="Continuous control with deep reinforcement learning" class="ltx_ref">32</a>, <a href="#bib.bib17" title="Addressing function approximation error in actor-critic methods" class="ltx_ref">18</a>]</cite> optimize a single objective by approximating gradients for gradient descent. <span class="ltx_text ltx_font_italic">Our key insight is to approximate objective and measure gradients for DQD algorithms by adapting ES and actor-critic methods.</span></p></div><div id="S1.p6" class="ltx_para"><p class="ltx_p">Our work makes three main contributions. <span class="ltx_text ltx_font_bold">(1)</span> We formalize the problem of quality diversity for reinforcement learning (QD-RL) and reduce it to an instance of DQD (Sec. <a href="#S2" title="2 Problem Statement ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). <span class="ltx_text ltx_font_bold">(2)</span> We develop two QD-RL variants of the DQD algorithm CMA-MEGA (Sec. <a href="#S4" title="4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). The first variant, CMA-MEGA (ES), approximates objective and measure gradients with ES. The second variant, CMA-MEGA (TD3, ES), approximates the objective gradient with TD3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="Addressing function approximation error in actor-critic methods" class="ltx_ref">18</a>]</cite> (an actor-critic method) and the measure gradients with ES. <span class="ltx_text ltx_font_bold">(3)</span> We benchmark our variants on four PyBullet locomotion tasks from QDGym <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="PyBullet gymperium" class="ltx_ref">14</a>, <a href="#bib.bib36" title="QDgym" class="ltx_ref">37</a>]</cite> (Sec. <a href="#S5" title="5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>-<a href="#S6" title="6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The first variant, CMA-MEGA (ES), achieves a QD score (Sec. <a href="#S5.SS1.SSS3" title="5.1.3 Metrics ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>) comparable to the state-of-the-art PGA-MAP-Elites in two tasks. The second variant, CMA-MEGA (TD3, ES), achieves comparable QD score with PGA-MAP-Elites in all tasks but is less efficient than PGA-MAP-Elites in two tasks. The code for these experiments is available at <a href="https://github.com/icaros-usc/dqd-rl" title="" class="ltx_ref ltx_font_typewriter ltx_url">https://github.com/icaros-usc/dqd-rl</a></p></div><div id="S1.p7" class="ltx_para"><p class="ltx_p">Our results contrast with prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="Differentiable quality diversity" class="ltx_ref">16</a>]</cite> where CMA-MEGA vastly outperforms OG-MAP-Elites, a DQD algorithm inspired by PGA-MAP-Elites, on benchmark functions where gradient information is available. Overall, we shed light on the limitations of CMA-MEGA in QD domains where the main challenge comes from optimizing the objective rather than from exploring measure space. At the same time, since we decouple gradient estimates from QD optimization, our work opens a path for future research that would benefit from independent improvements to either DQD or RL.</p></div></section><section id="S2" class="ltx_section"><h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Problem Statement</h2><section id="S2.SS1" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Quality Diversity (QD)</h3><div id="S2.SS1.p1" class="ltx_para"><p class="ltx_p">We adopt the definition of QD from prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="Differentiable quality diversity" class="ltx_ref">16</a>]</cite>. For a solution vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span>, QD considers an objective function <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span> measures<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup> <span class="ltx_tag ltx_tag_note">1</span> Prior work has also referred to measures as “behavior characteristics” or “behavior descriptors.”</span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo><mo>∈</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">m_{i}({\bm{\phi}})\in\mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68889em;vertical-align:0"></span><span class="mord"><span class="mord mathbb">R</span></span></span></span></span> (for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><mn>1..</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">i\in 1..k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69862em;vertical-align:-.0391em"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span>) or, as a joint measure, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">{\bm{m}}({\bm{\phi}})\in\mathbb{R}^{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.849108em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.849108em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.03148em">k</span></span></span></span></span></span></span></span></span></span></span></span>. These measures form a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span>-dimensional measure space <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.14643em">X</span></span></span></span></span>. For every <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">x</mi><mo>∈</mo><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">{\bm{x}}\in\mathcal{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.14643em">X</span></span></span></span></span>, the QD objective is to find solution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span> such that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">x</mi></mrow><annotation encoding="application/x-tex">{\bm{m}}({\bm{\phi}})={\bm{x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> is maximized. Since <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.14643em">X</span></span></span></span></span> is continuous, it would require infinite memory to solve the QD problem, so algorithms in the MAP-Elites family <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="Illuminating search spaces by mapping elites" class="ltx_ref">34</a>, <a href="#bib.bib7" title="Robots that can adapt like animals" class="ltx_ref">12</a>]</cite> discretize <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.14643em">X</span></span></span></span></span> by forming a tesselation <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.78055em;vertical-align:-.09722em"></span><span class="mord"><span class="mord mathcal" style="margin-right:.08222em">Y</span></span></span></span></span> consisting of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span> cells. Thus, we relax the QD problem to one of searching for an <span class="ltx_text ltx_font_italic">archive</span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal">A</span></span></span></span></span> consisting of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span> <span class="ltx_text ltx_font_italic">elites</span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\phi}}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93858em;vertical-align:-.24414em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.21752399999999997em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span>, one for each cell in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.78055em;vertical-align:-.09722em"></span><span class="mord"><span class="mord mathcal" style="margin-right:.08222em">Y</span></span></span></span></span>. Then, the QD objective is to maximize the performance <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f({\bm{\phi}}_{i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.21752399999999997em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> of all elites:</p><table id="A6.EGx1" class="ltx_eqn_align ltx_eqn_table ltx_equationgroup"><tbody id="S2.E1"><tr class="ltx_align_baseline ltx_eqn_row ltx_equation"><td class="ltx_eqn_center_padleft ltx_eqn_cell"></td><td class="ltx_td ltx_align_right ltx_eqn_cell" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><munder><mo><mi>max</mi><mo>⁡</mo></mo><msub><mi mathvariant="bold-italic">ϕ</mi><mrow><mn>1..</mn><mi>M</mi></mrow></msub></munder><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle\max_{{\bm{\phi}}_{1..M}}\sum_{i=1}^{M}f({\bm{\phi}}_{i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.106005em;vertical-align:-1.277669em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.43055999999999994em"><span style="top:-2.347892em;margin-left:0"><span class="pstrut" style="height:3em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2222242857142857em"><span style="top:-2.2341314285714287em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">.</span><span class="mord mtight">.</span><span class="mord mtight mathnormal" style="margin-right:.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.26586857142857145em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.9382159999999999em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span><span class="mtight mrel">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.21752399999999997em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></td><td class="ltx_eqn_center_padright ltx_eqn_cell"></td><td rowspan="1" class="ltx_align_right ltx_eqn_cell ltx_align_middle ltx_eqn_eqno"><span class="ltx_tag ltx_align_right ltx_tag_equation">(1)</span></td></tr></tbody></table></div><section id="S2.SS1.SSS1" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Differentiable Quality Diversity (DQD)</h4><div id="S2.SS1.SSS1.p1" class="ltx_para"><p class="ltx_p">In DQD, we assume <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi></mrow><annotation encoding="application/x-tex">{\bm{m}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span></span></span></span> are first-order differentiable. We denote the objective gradient as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><mi>f</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{f}({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10764em">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> and the measure gradients as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><mi mathvariant="bold-italic">m</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{\bm{m}}({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.161108em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">m</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> or <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><msub><mi>m</mi><mi>i</mi></msub></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{m_{i}}({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0001em;vertical-align:-.2501em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139199999999997em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3280857142857143em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span>.</p></div></section></section><section id="S2.SS2" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>Quality Diversity for Reinforcement Learning (QD-RL)</h3><div id="S2.SS2.p1" class="ltx_para"><p class="ltx_p">We define QD-RL as an instance of the QD problem in which the objective <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> is the <span class="ltx_text ltx_font_italic">expected discounted return</span> of an RL policy, and the measures <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{m}}({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> are functions of the policy. Formally, drawing on the Markov Decision Process (MDP) formulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="Reinforcement learning: an introduction" class="ltx_ref">45</a>]</cite>, we represent QD-RL as a tuple <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="script">S</mi><mo separator="true">,</mo><mi mathvariant="script">U</mi><mo separator="true">,</mo><mi>p</mi><mo separator="true">,</mo><mi>r</mi><mo separator="true">,</mo><mi>γ</mi><mo separator="true">,</mo><mi mathvariant="bold-italic">m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathcal{S},\mathcal{U},p,r,\gamma,{\bm{m}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathcal" style="margin-right:.075em">S</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathcal" style="margin-right:.09931em">U</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05556em">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span><span class="mclose">)</span></span></span></span>. On discrete timesteps <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.61508em;vertical-align:0"></span><span class="mord mathnormal">t</span></span></span></span> in an episode of interaction, an agent observes state <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∈</mo><mi mathvariant="script">S</mi></mrow><annotation encoding="application/x-tex">s\in\mathcal{S}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.075em">S</span></span></span></span></span> and takes action <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∈</mo><mi mathvariant="script">U</mi></mrow><annotation encoding="application/x-tex">a\in\mathcal{U}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.09931em">U</span></span></span></span></span> according to a policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi mathvariant="bold-italic">ϕ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\bm{\phi}}(a|s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span> with parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span>. The agent then receives scalar reward <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r(s,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span> and observes next state <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∈</mo><mi mathvariant="script">S</mi></mrow><annotation encoding="application/x-tex">s^{\prime}\in\mathcal{S}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.790992em;vertical-align:-.0391em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.751892em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.075em">S</span></span></span></span></span> according to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∼</mo><mi>p</mi><mo stretchy="false">(</mo><mo>⋅</mo><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s^{\prime}\sim p(\cdot|s,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.751892em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.751892em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span>. Each episode thus has a trajectory <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\xi=\{s_{0},a_{0},s_{1},a_{1},..,s_{T}\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.04601em">ξ</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.13889em">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.13889em">T</span></span></span></span> is the number of timesteps in the episode, and the probability that policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi mathvariant="bold-italic">ϕ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> takes trajectory <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi></mrow><annotation encoding="application/x-tex">\xi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.04601em">ξ</span></span></span></span> is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi mathvariant="bold-italic">ϕ</mi></msub><mo stretchy="false">(</mo><mi>ξ</mi><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false">)</mo><msubsup><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>π</mi><mi mathvariant="bold-italic">ϕ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_{\bm{\phi}}(\xi)=p(s_{0})\prod_{t=0}^{T-1}\pi_{\bm{\phi}}(a_{t}|s_{t})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.04601em">ξ</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.2809409999999999em;vertical-align:-.29971000000000003em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-.0000050000000000050004em">∏</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.981231em"><span style="top:-2.40029em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span><span class="mtight mrel">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.2029em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.13889em">T</span><span class="mtight mbin">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.29971000000000003em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}|s_{t},a_{t})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.301108em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span><span class="mtight mbin">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>. Now, we define the <span class="ltx_text ltx_font_italic">expected discounted return</span> of policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi mathvariant="bold-italic">ϕ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> as</p><table id="A6.EGx2" class="ltx_eqn_align ltx_eqn_table ltx_equationgroup"><tbody id="S2.E2"><tr class="ltx_align_baseline ltx_eqn_row ltx_equation"><td class="ltx_eqn_center_padleft ltx_eqn_cell"></td><td class="ltx_td ltx_align_right ltx_eqn_cell" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>ξ</mi><mo>∼</mo><msub><mi>p</mi><mi mathvariant="bold-italic">ϕ</mi></msub></mrow></msub><mrow><mo fence="true">[</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msup><mi>γ</mi><mi>t</mi></msup><mi>r</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo fence="true">]</mo></mrow></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle f({\bm{\phi}})=\mathbb{E}_{\xi\sim p_{\bm{\phi}}}\left[\sum_{t=0% }^{T}\gamma^{t}r(s_{t},a_{t})\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:3.0954490000000003em;vertical-align:-1.267113em"></span><span class="mord"><span class="mord"><span class="mord mathbb">E</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.04601em">ξ</span><span class="mtight mrel">∼</span><span class="mord mtight"><span class="mord mtight mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3487714285714287em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.29011428571428566em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35307999999999995em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0"><span class="delimsizing size4">[</span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em"><span style="top:-1.882887em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span><span class="mtight mrel">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.13889em">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05556em">γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.843556em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0"><span class="delimsizing size4">]</span></span></span></span></span></span></td><td class="ltx_eqn_center_padright ltx_eqn_cell"></td><td rowspan="1" class="ltx_align_right ltx_eqn_cell ltx_align_middle ltx_eqn_eqno"><span class="ltx_tag ltx_align_right ltx_tag_equation">(2)</span></td></tr></tbody></table><p class="ltx_p">where the discount factor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>∈</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\gamma\in(0,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7335400000000001em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.05556em">γ</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span> trades off between short- and long-term rewards. Finally, each policy is characterized by a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span>-dimensional measure function <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{m}}({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span>.</p></div><section id="S2.SS2.SSS1" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>QD-RL as an instance of DQD</h4><div id="S2.SS2.SSS1.p1" class="ltx_para"><p class="ltx_p">We reduce QD-RL to a DQD problem. Since the exact gradients <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{f}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.972218em;vertical-align:-.286108em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10764em">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><mi mathvariant="bold-italic">m</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{\bm{m}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83611em;vertical-align:-.15em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.161108em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">m</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> usually do not exist in QD-RL, we must instead approximate them.</p></div></section></section></section><section id="S3" class="ltx_section"><h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Background</h2><section id="S3.SS1" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Single-Objective Reinforcement Learning</h3><div id="S3.SS1.p1" class="ltx_para"><p class="ltx_p">We review algorithms which train a policy to maximize a single objective, i.e. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> in Eq. <a href="#S2.E2" title="(2) ‣ 2.2 Quality Diversity for Reinforcement Learning (QD-RL) ‣ 2 Problem Statement ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, with the goal of applying these algorithms’ gradient approximations to DQD in Sec. <a href="#S4" title="4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p></div><section id="S3.SS1.SSS1" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Evolution strategies (ES)</h4><div id="S3.SS1.SSS1.p1" class="ltx_para"><p class="ltx_p">ES <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="Evolution strategies – a comprehensive introduction" class="ltx_ref">4</a>]</cite> is a class of evolutionary algorithms which optimizes the objective by sampling a population of solutions and moving the population towards areas of higher performance. Natural Evolution Strategies (NES) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="Natural evolution strategies" class="ltx_ref">49</a>, <a href="#bib.bib60" title="Natural evolution strategies" class="ltx_ref">50</a>]</cite> is a type of ES which updates the sampling distribution of solutions by taking steps on distribution parameters in the direction of the natural gradient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="Natural Gradient Works Efficiently in Learning" class="ltx_ref">2</a>]</cite>. For example, with a Gaussian sampling distribution, each iteration of an NES would compute natural gradients to update the mean <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">μ</mi></mrow><annotation encoding="application/x-tex">{\bm{\mu}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.63888em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span></span></span></span> and covariance <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Σ</mi></mrow><annotation encoding="application/x-tex">{\bm{\Sigma}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68611em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">Σ</span></span></span></span></span></span></span>.</p></div><div id="S3.SS1.SSS1.p2" class="ltx_para"><p class="ltx_p">We consider two NES-inspired approaches which have demonstrated success in RL domains. First, prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="Evolution strategies as a scalable alternative to reinforcement learning" class="ltx_ref">41</a>]</cite> introduces an algorithm which, drawing from NES, samples <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mrow><mi>e</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\lambda_{es}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> solutions from an isotropic Gaussian but only computes a gradient step for the mean <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span>. We refer to this algorithm as OpenAI-ES. Each solution sampled by OpenAI-ES is represented as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi><mo>+</mo><mi>σ</mi><msub><mi mathvariant="bold-italic">ϵ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\phi}}+\sigma{\bm{\epsilon}}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.59444em;vertical-align:-.15em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϵ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span></span></span></span> is the fixed standard deviation of the Gaussian and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">ϵ</mi><mi>i</mi></msub><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn mathvariant="bold">0</mn><mo separator="true">,</mo><mi mathvariant="bold-italic">I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{\epsilon}}_{i}\sim{\mathcal{N}}(\mathbf{0},{\bm{I}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.59444em;vertical-align:-.15em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϵ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:.14736em">N</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">0</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.07778em">I</span></span></span></span><span class="mclose">)</span></span></span></span>. Once these solutions are evaluated, OpenAI-ES estimates the gradient as</p><table id="A6.EGx3" class="ltx_eqn_align ltx_eqn_table ltx_equationgroup"><tbody id="S3.E3"><tr class="ltx_align_baseline ltx_eqn_row ltx_equation"><td class="ltx_eqn_center_padleft ltx_eqn_cell"></td><td class="ltx_td ltx_align_right ltx_eqn_cell" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">∇</mi><mi>f</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo><mo>≈</mo><mfrac><mn>1</mn><mrow><msub><mi>λ</mi><mrow><mi>e</mi><mi>s</mi></mrow></msub><mi>σ</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>λ</mi><mrow><mi>e</mi><mi>s</mi></mrow></msub></munderover><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo>+</mo><mi>σ</mi><msub><mi mathvariant="bold-italic">ϵ</mi><mi>i</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold-italic">ϵ</mi><mi>i</mi></msub></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle{\bm{\nabla}}_{f}({\bm{\phi}})\approx\frac{1}{\lambda_{es}\sigma}% \sum_{i=1}^{\lambda_{es}}f({\bm{\phi}}+\sigma{\bm{\epsilon}}_{i}){\bm{\epsilon% }}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10764em">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:3.1248820000000004em;vertical-align:-1.277669em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.3139999999999996em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.8360000000000001em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8472130000000004em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span><span class="mtight mrel">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.311105em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.16454285714285719em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϵ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϵ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_eqn_center_padright ltx_eqn_cell"></td><td rowspan="1" class="ltx_align_right ltx_eqn_cell ltx_align_middle ltx_eqn_eqno"><span class="ltx_tag ltx_align_right ltx_tag_equation">(3)</span></td></tr></tbody></table><p class="ltx_p">OpenAI-ES then passes this estimate to an Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="Adam: A method for stochastic optimization" class="ltx_ref">26</a>]</cite> which outputs a gradient ascent step for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span>. To make the estimate more accurate, OpenAI-ES further includes techniques such as mirror sampling and rank normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="A visual guide to evolution strategies" class="ltx_ref">20</a>, <a href="#bib.bib20" title="Natural evolution strategies" class="ltx_ref">49</a>]</cite>.</p></div><div id="S3.SS1.SSS1.p3" class="ltx_para"><p class="ltx_p">Another approach is Augmented Random Search (ARS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="Simple random search of static linear policies is competitive for reinforcement learning" class="ltx_ref">33</a>]</cite>, which simplifies OpenAI-ES by removing rank normalization and Adam and scaling gradient ascent steps by the standard deviation of the sampled solutions’ objective values. We focus on OpenAI-ES over ARS because ARS was initially designed for linear policies, while our work trains neural network policies.</p></div></section><section id="S3.SS1.SSS2" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Actor-critic methods</h4><div id="S3.SS1.SSS2.p1" class="ltx_para"><p class="ltx_p">While ES treats the objective as a black box, actor-critic methods leverage the MDP structure of the objective, i.e. the fact that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> is a sum of Markovian values. We are most interested in Twin Delayed Deep Deterministic policy gradient (TD3) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="Addressing function approximation error in actor-critic methods" class="ltx_ref">18</a>]</cite>, an off-policy actor-critic method. TD3 maintains (1) an actor consisting of the policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi mathvariant="bold-italic">ϕ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> and (2) a critic consisting of state-action value functions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msub><mi mathvariant="bold-italic">θ</mi><mn>1</mn></msub></msub><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}_{1}}(s,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0001em;vertical-align:-.2501em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31731428571428577em"><span style="top:-2.357em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msub><mi mathvariant="bold-italic">θ</mi><mn>2</mn></msub></msub><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}_{2}}(s,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0001em;vertical-align:-.2501em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31731428571428577em"><span style="top:-2.357em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span> which differ only in random initialization. Through interactions in the environment, the actor generates experience which is stored in a replay buffer <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span></span></span></span>. This experience is sampled to train <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msub><mi mathvariant="bold-italic">θ</mi><mn>1</mn></msub></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}_{1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93343em;vertical-align:-.2501em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31731428571428577em"><span style="top:-2.357em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msub><mi mathvariant="bold-italic">θ</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}_{2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93343em;vertical-align:-.2501em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31731428571428577em"><span style="top:-2.357em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span>. Simultaneously, the actor improves by maximizing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msub><mi mathvariant="bold-italic">θ</mi><mn>1</mn></msub></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}_{1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93343em;vertical-align:-.2501em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31731428571428577em"><span style="top:-2.357em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span> via gradient ascent (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msub><mi mathvariant="bold-italic">θ</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}_{2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93343em;vertical-align:-.2501em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31731428571428577em"><span style="top:-2.357em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span> is only used during critic training). Specifically, for an objective <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">f^{\prime}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.946332em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.751892em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> which is based on the critic and approximates <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span>, TD3 estimates a gradient <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{f^{\prime}}({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6828285714285715em"><span style="top:-2.786em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> and passes it to an Adam optimizer. Notably, TD3 never updates network weights directly, instead accumulating weights into <span class="ltx_text ltx_font_italic">target networks</span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><msup><mi mathvariant="bold-italic">ϕ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub></mrow><annotation encoding="application/x-tex">\pi_{{\bm{\phi}}^{\prime}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.738756em;vertical-align:-.3081959999999999em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.5279120000000006em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.7384114285714285em"><span style="top:-2.841582857142857em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3081959999999999em"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msubsup><mi mathvariant="bold-italic">θ</mi><mn>1</mn><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}^{\prime}_{1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.07573em;vertical-align:-.3924000000000001em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.52566em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7416285714285715em"><span style="top:-2.188485714285714em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span style="top:-2.8448em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.31151428571428574em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3924000000000001em"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msubsup><mi mathvariant="bold-italic">θ</mi><mn>2</mn><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}^{\prime}_{2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.07573em;vertical-align:-.3924000000000001em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.52566em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7416285714285715em"><span style="top:-2.188485714285714em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-2.8448em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.31151428571428574em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3924000000000001em"><span></span></span></span></span></span></span></span></span></span> via an exponentially weighted moving average with update rate <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.1132em">τ</span></span></span></span>.</p></div></section></section><section id="S3.SS2" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Quality Diversity Algorithms</h3><section id="S3.SS2.SSS1" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>MAP-Elites extensions for QD-RL</h4><div id="S3.SS2.SSS1.p1" class="ltx_para"><p class="ltx_p">One of the simplest QD algorithms is MAP-Elites <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="Illuminating search spaces by mapping elites" class="ltx_ref">34</a>, <a href="#bib.bib7" title="Robots that can adapt like animals" class="ltx_ref">12</a>]</cite>. MAP-Elites creates an archive <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal">A</span></span></span></span></span> by tesselating the measure space <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.14643em">X</span></span></span></span></span> into a grid of evenly-sized cells. Then, it draws <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span> initial solutions from a multivariate Gaussian <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">ϕ</mi><mn mathvariant="bold">0</mn></msub><mo separator="true">,</mo><mi>σ</mi><mi mathvariant="bold-italic">I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(\mathbf{{\bm{\phi}}_{0}},\sigma{\bm{I}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathcal" style="margin-right:.14736em">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20696799999999996em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathbf">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.07778em">I</span></span></span></span><span class="mclose">)</span></span></span></span> centered at some <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">{\bm{\phi}}_{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93858em;vertical-align:-.24414em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20696799999999996em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span>. Next, for each sampled solution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span>, MAP-Elites computes <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{m}}({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> and inserts <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span> into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal">A</span></span></span></span></span>. In subsequent iterations, MAP-Elites randomly selects <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span> solutions from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal">A</span></span></span></span></span> and adds Gaussian noise, i.e. solution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span> becomes <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi><mo>+</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn mathvariant="bold">0</mn><mo separator="true">,</mo><mi>σ</mi><mi mathvariant="bold-italic">I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{\phi}}+\mathcal{N}(\mathbf{0},\sigma{\bm{I}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathcal" style="margin-right:.14736em">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">0</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.07778em">I</span></span></span></span><span class="mclose">)</span></span></span></span>. Solutions are placed into cells based on their measures; if a solution has higher <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> than the solution currently in the cell, it replaces that solution. Once inserted into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal">A</span></span></span></span></span>, solutions are known as <span class="ltx_text ltx_font_italic">elites</span>.</p></div><div id="S3.SS2.SSS1.p2" class="ltx_para"><p class="ltx_p">Due to the high dimensionality of neural network parameters, MAP-Elites has not proven effective for QD-RL. Hence, several extensions merge MAP-Elites with actor-critic methods and ES. For instance, Policy Gradient Assisted MAP-Elites (PGA-MAP-Elites) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="Policy gradient assisted map-elites" class="ltx_ref">36</a>]</cite> combines MAP-Elites with TD3. Each iteration, PGA-MAP-Elites evaluates <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span> solutions for insertion into the archive. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>λ</mi><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{\lambda}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8801079999999999em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">λ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> of these are created by selecting random solutions from the archive and taking gradient ascent steps with a TD3 critic. The other <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>λ</mi><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{\lambda}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8801079999999999em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">λ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> solutions are created with a directional variation operator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="Discovering the elite hypervolume by leveraging interspecies correlation" class="ltx_ref">48</a>]</cite> which selects two solutions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">{\bm{\phi}}_{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93858em;vertical-align:-.24414em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20696799999999996em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">{\bm{\phi}}_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93858em;vertical-align:-.24414em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20696799999999996em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span> from the archive and creates a new one according to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><msub><mi mathvariant="bold-italic">ϕ</mi><mn>1</mn></msub><mo>+</mo><msub><mi>σ</mi><mn>1</mn></msub><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn mathvariant="bold">0</mn><mo separator="true">,</mo><mi mathvariant="bold-italic">I</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>σ</mi><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">ϕ</mi><mn>2</mn></msub><mo>−</mo><msub><mi mathvariant="bold-italic">ϕ</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{\prime}={\bm{\phi}}_{1}+\sigma_{1}\mathcal{N}(\mathbf{0},{\bm{I}}% )+\sigma_{2}({\bm{\phi}}_{2}-{\bm{\phi}}_{1})\mathcal{N}(0,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.030672em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.93858em;vertical-align:-.24414em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20696799999999996em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathcal" style="margin-right:.14736em">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">0</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.07778em">I</span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20696799999999996em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20696799999999996em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathcal" style="margin-right:.14736em">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>. PGA-MAP-Elites achieves state-of-the-art performance on locomotion tasks in the QDGym benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="QDgym" class="ltx_ref">37</a>]</cite>.</p></div><div id="S3.SS2.SSS1.p3" class="ltx_para"><p class="ltx_p">Another MAP-Elites extension is ME-ES <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="Scaling map-elites to deep neuroevolution" class="ltx_ref">8</a>]</cite>, which combines MAP-Elites with an OpenAI-ES optimizer. In the “explore-exploit” variant, ME-ES alternates between two phases. In the “exploit” phase, ME-ES restarts OpenAI-ES at a mean <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span> and optimizes the objective for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span> iterations, inserting the current <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span> into the archive in each iteration. In the “explore” phase, ME-ES repeats this process, but OpenAI-ES instead optimizes for novelty, where novelty is the distance in measure space from a new solution to previously encountered solutions. ME-ES also has an “exploit” variant and an “explore” variant, which each execute only one type of phase.</p></div><div id="S3.SS2.SSS1.p4" class="ltx_para"><p class="ltx_p">Our work is related to ME-ES in that we also adapt OpenAI-ES, but instead of alternating between following a novelty gradient and objective gradient, we compute all objective and measure gradients and allow a CMA-ES <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="The CMA evolution strategy: A tutorial" class="ltx_ref">22</a>]</cite> instance to decide which gradients to follow to maximize QD score. We include MAP-Elites, PGA-MAP-Elites, and ME-ES as baselines in our experiments.</p></div></section><section id="S3.SS2.SSS2" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Covariance Matrix Adaptation MAP-Elites via a Gradient Arborescence (CMA-MEGA)</h4><div id="S3.SS2.SSS2.p1" class="ltx_para"><p class="ltx_p">We directly extend CMA-MEGA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="Differentiable quality diversity" class="ltx_ref">16</a>]</cite> to address QD-RL. CMA-MEGA is a DQD algorithm based on the QD algorithm CMA-ME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="Covariance matrix adaptation for the rapid illumination of behavior space" class="ltx_ref">17</a>]</cite>. The intuition behind CMA-MEGA is that for solution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span>, we can simultaneously increase/decrease both the objective and measures by following the objective and measure gradients of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ϕ</mi></mrow><annotation encoding="application/x-tex">{\bm{\phi}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span></span></span></span>. In doing so, we traverse <span class="ltx_text ltx_font_italic">objective-measure space</span> and generate solutions which maximize improvement of the archive.</p></div><div id="S3.SS2.SSS2.p2" class="ltx_para"><p class="ltx_p">Each iteration, CMA-MEGA first calculates objective and measure gradients for a mean solution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.967476em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span>. Next, it generates <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span> new solutions by sampling gradient coefficients <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">c</mi><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">μ</mi><mo separator="true">,</mo><mi mathvariant="bold">Σ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{c}}\sim\mathcal{N}({\bm{\mu}},{\bm{\Sigma}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">c</span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathcal" style="margin-right:.14736em">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">Σ</span></span></span></span><span class="mclose">)</span></span></span></span> and computing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>←</mo><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup><mo>+</mo><msub><mi mathvariant="bold-italic">c</mi><mn>0</mn></msub><msub><mi mathvariant="bold">∇</mi><mi>f</mi></msub><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup><mo stretchy="false">)</mo><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><msub><mi mathvariant="bold-italic">c</mi><mi>j</mi></msub><msub><mi mathvariant="bold">∇</mi><msub><mi>m</mi><mi>j</mi></msub></msub><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{\prime}\leftarrow{\bm{\phi}}^{*}+{\bm{c}}_{0}{\bm{\nabla}}_{f}({% \bm{\phi}}^{*})+\sum_{j=1}^{k}{\bm{c}}_{j}{\bm{\nabla}}_{m_{j}}({\bm{\phi}}^{*})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.030672em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.967476em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.0591439999999999em;vertical-align:-.286108em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">c</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10764em">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.424826em;vertical-align:-.43581800000000004em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-.0000050000000000050004em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.9890079999999999em"><span style="top:-2.40029em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.05724em">j</span><span class="mtight mrel">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.03148em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.43581800000000004em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">c</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3280857142857143em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2818857142857143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.34731999999999996em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>. CMA-MEGA inserts these solutions into the archive and computes their <span class="ltx_text ltx_font_italic">improvement,</span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Δ</span></span></span></span>. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Δ</span></span></span></span> is defined as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">ϕ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f({\bm{\phi}}^{\prime})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0862319999999999em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> if <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{\prime}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.030672em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> populates a new cell, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">ϕ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><msubsup><mi mathvariant="bold-italic">ϕ</mi><mi mathvariant="script">E</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f({\bm{\phi}}^{\prime})-f({\bm{\phi}}^{\prime}_{\mathcal{E}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0862319999999999em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.0862319999999999em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-2.4530000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight mathcal" style="margin-right:.08944em">E</span></span></span></span></span><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> if <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{\prime}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.030672em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> improves an existing cell (replaces a previous solution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold-italic">ϕ</mi><mi mathvariant="script">E</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{\prime}_{\mathcal{E}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.083232em;vertical-align:-.247em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-2.4530000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight mathcal" style="margin-right:.08944em">E</span></span></span></span></span><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span>). After CMA-MEGA inserts the solutions, it ranks them by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Δ</span></span></span></span>. If a solution populates a new cell, its <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Δ</span></span></span></span> always ranks higher than that of a solution which only improves an existing cell. Finally, CMA-MEGA passes the ranking to CMA-ES <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="The CMA evolution strategy: A tutorial" class="ltx_ref">22</a>]</cite>, which adapts <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">μ</mi></mrow><annotation encoding="application/x-tex">{\bm{\mu}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.63888em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Σ</mi></mrow><annotation encoding="application/x-tex">{\bm{\Sigma}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68611em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">Σ</span></span></span></span></span></span></span> such that future gradient coefficients <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">c</mi></mrow><annotation encoding="application/x-tex">{\bm{c}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">c</span></span></span></span></span></span></span> are more likely to generate archive improvement. By leveraging gradient information, CMA-MEGA solves QD benchmarks with orders of magnitude fewer solution evaluations than previous QD algorithms.</p></div></section><section id="S3.SS2.SSS3" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Beyond MAP-Elites</h4><div id="S3.SS2.SSS3.p1" class="ltx_para"><p class="ltx_p">Several QD-RL algorithms have been developed outside the MAP-Elites family. NS-ES <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents" class="ltx_ref">10</a>]</cite> builds on Novelty Search (NS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="Abandoning Objectives: Evolution Through the Search for Novelty Alone" class="ltx_ref">29</a>, <a href="#bib.bib42" title="Evolving a diversity of virtual creatures through novelty search and local competition" class="ltx_ref">30</a>]</cite>, a family of QD algorithms which add solutions to an <span class="ltx_text ltx_font_italic">unstructured archive</span> only if they are far away from existing archive solutions in measure space. Using OpenAI-ES, NS-ES concurrently optimizes several agents for novelty. Its variants NSR-ES and NSRA-ES optimize for a linear combination of novelty and objective. Meanwhile, the QD-RL algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="QD-RL: efficient mixing of quality and diversity in reinforcement learning" class="ltx_ref">6</a>]</cite> (distinct from the QD-RL problem we define) maintains an archive with all past solutions and optimizes agents along a Pareto front of the objective and novelty. As NS-ES and QD-RL do not output a MAP-Elites grid archive, we leave their investigation for future work.</p></div></section></section><section id="S3.SS3" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.3 </span>Diversity in Reinforcement Learning</h3><div id="S3.SS3.p1" class="ltx_para"><p class="ltx_p">Here we distinguish QD-RL from prior work which also applies diversity to RL. One area of work is in latent- and goal-conditioned policies. For latent-conditioned policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi mathvariant="bold-italic">ϕ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\bm{\phi}}(a|s,z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.04398em">z</span><span class="mclose">)</span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="Diversity is all you need: learning skills without a reward function" class="ltx_ref">15</a>, <a href="#bib.bib28" title="One solution is not all you need: few-shot extrapolation via structured maxent rl" class="ltx_ref">27</a>, <a href="#bib.bib29" title="InfoGAIL: interpretable imitation learning from visual demonstrations" class="ltx_ref">31</a>]</cite> or goal-conditioned policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi mathvariant="bold-italic">ϕ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>g</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\bm{\phi}}(a|s,g)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mclose">)</span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="Universal value function approximators" class="ltx_ref">42</a>, <a href="#bib.bib30" title="Hindsight experience replay" class="ltx_ref">3</a>]</cite>, varying the latent variable <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.04398em">z</span></span></span></span> or goal <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">g</span></span></span></span> results in different behaviors, e.g. different walking gaits or walking to a different location. While QD-RL also seeks a range of behaviors, QD-RL algorithms <span class="ltx_text ltx_font_italic">observe</span> the measures <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\bm{m}}({\bm{\phi}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="mclose">)</span></span></span></span> instead of <span class="ltx_text ltx_font_italic">conditioning</span> on some <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">m</mi></mrow><annotation encoding="application/x-tex">{\bm{m}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">m</span></span></span></span></span></span></span>. Hence, QD-RL outputs an archive of nonconditioned policies rather than a single conditioned policy.</p></div><div id="S3.SS3.p2" class="ltx_para"><p class="ltx_p">Another area of work combines evolutionary and actor-critic algorithms to solve single-objective hard-exploration problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="GEP-PG: decoupling exploration and exploitation in deep reinforcement learning algorithms" class="ltx_ref">9</a>, <a href="#bib.bib34" title="Evolution-guided policy gradient in reinforcement learning" class="ltx_ref">25</a>, <a href="#bib.bib33" title="CEM-RL: combining evolutionary and gradient-based methods for policy search" class="ltx_ref">39</a>, <a href="#bib.bib47" title="Guiding evolutionary strategies with off-policy actor-critic" class="ltx_ref">46</a>, <a href="#bib.bib51" title="Collaborative evolutionary reinforcement learning" class="ltx_ref">24</a>]</cite>. In these methods, an evolutionary algorithm such as cross-entropy method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="A tutorial on the cross-entropy method" class="ltx_ref">13</a>]</cite> facilitates exploration by generating a diverse population of policies, while an actor-critic algorithm such as TD3 trains high-performing policies with this population’s environment experience. QD-RL differs from these methods in that it views diversity as a component of the output, while these methods view diversity as a means for environment exploration. Hence, QD-RL measures diversity via a measure function and stores the solutions in an archive. In contrast, these methods do not need an explicit measure of diversity, as they assume that the policies in their populations are sufficiently “different” that they can drive exploration and discover the optimal single objective.</p></div></section></section><section id="S4" class="ltx_section"><h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Approximating Gradients for CMA-MEGA</h2><div id="S4.p1" class="ltx_para"><p class="ltx_p">Since CMA-MEGA requires exact objective and measure gradients, we cannot directly apply it to QD-RL. To address this limitation, we replace exact gradients with gradient approximations (Sec. <a href="#S4.SS1" title="4.1 Approximating Objective and Measure Gradients ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and develop two CMA-MEGA variants (Sec. <a href="#S4.SS2" title="4.2 CMA-MEGA Variants ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p></div><section id="S4.SS1" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Approximating Objective and Measure Gradients</h3><div id="S4.SS1.p1" class="ltx_para"><p class="ltx_p">We adapt gradient approximations from ES and actor-critic methods. Since the objective has an MDP structure, we estimate objective gradients <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{f}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.972218em;vertical-align:-.286108em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10764em">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> with ES and actor-critic methods. Since the measures are black boxes, we estimate measure gradients <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><mi mathvariant="bold-italic">m</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{\bm{m}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83611em;vertical-align:-.15em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.161108em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">m</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> with ES.</p></div><section id="S4.SS1.SSS1" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Approximating objective gradients with ES and actor-critic methods</h4><div id="S4.SS1.SSS1.p1" class="ltx_para"><p class="ltx_p">We estimate objective gradients with two methods. First, we treat the objective as a black box and estimate its gradient with a black box method, i.e. the OpenAI-ES gradient estimate in Eq. <a href="#S3.E3" title="(3) ‣ 3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Since OpenAI-ES performs well in RL domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="Evolution strategies as a scalable alternative to reinforcement learning" class="ltx_ref">41</a>, <a href="#bib.bib21" title="Efficacy of modern neuro-evolutionary strategies for continuous control optimization" class="ltx_ref">38</a>, <a href="#bib.bib22" title="ES is more than just a traditional finite-difference approximator" class="ltx_ref">28</a>]</cite>, we believe this estimate is suitable for approximating gradients for CMA-MEGA in QD-RL settings. Importantly, this estimate requires environment interaction by evaluating <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mrow><mi>e</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\lambda_{es}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> solutions.</p></div><div id="S4.SS1.SSS1.p2" class="ltx_para"><p class="ltx_p">Since the objective has a well-defined structure, i.e. it is a sum of rewards from an MDP (Eq. <a href="#S2.E2" title="(2) ‣ 2.2 Quality Diversity for Reinforcement Learning (QD-RL) ‣ 2 Problem Statement ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), we also estimate its gradient with an actor-critic method, TD3. TD3 is well-suited for this purpose because it efficiently estimates objective gradients for the multiple policies that CMA-MEGA and other QD-RL algorithms generate. In particular, once the critic is trained, TD3 can provide a gradient estimate for any policy without additional environment interaction.</p></div><div id="S4.SS1.SSS1.p3" class="ltx_para"><p class="ltx_p">Among actor-critic methods, we select TD3 since it achieves high performance while optimizing primarily for the RL objective. Prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="Addressing function approximation error in actor-critic methods" class="ltx_ref">18</a>]</cite> shows that TD3 outperforms on-policy methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="Trust region policy optimization" class="ltx_ref">43</a>, <a href="#bib.bib14" title="Proximal policy optimization algorithms" class="ltx_ref">44</a>]</cite>. While the off-policy Soft Actor-Critic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor" class="ltx_ref">21</a>]</cite> algorithm can outperform TD3, it optimizes a maximum-entropy objective designed to encourage exploration. In our work, this exploration is unnecessary because QD algorithms already search for diverse solutions.</p></div></section><section id="S4.SS1.SSS2" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Approximating measure gradients with ES</h4><div id="S4.SS1.SSS2.p1" class="ltx_para"><p class="ltx_p">Since we treat measures as black boxes (Sec. <a href="#S2.SS2" title="2.2 Quality Diversity for Reinforcement Learning (QD-RL) ‣ 2 Problem Statement ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>), we can only estimate their gradient with black box methods. Thus, similar to the objective, we approximate each individual measure’s gradient <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><msub><mi>m</mi><mi>i</mi></msub></msub></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{m_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93621em;vertical-align:-.2501em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139199999999997em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3280857142857143em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span> with the OpenAI-ES gradient estimate, replacing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">m_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> in Eq. <a href="#S3.E3" title="(3) ‣ 3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p></div><div id="S4.SS1.SSS2.p2" class="ltx_para"><p class="ltx_p">Since the OpenAI-ES gradient estimate requires additional environment interaction, all of our CMA-MEGA variants require environment interaction to estimate gradients. However, the environment interaction required to estimate measure gradients remains constant even as the number of measures increases, since we can reuse the same <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mrow><mi>e</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\lambda_{es}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> solutions to estimate each <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><msub><mi>m</mi><mi>i</mi></msub></msub></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{m_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93621em;vertical-align:-.2501em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139199999999997em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3280857142857143em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span>.</p></div><div id="S4.SS1.SSS2.p3" class="ltx_para"><p class="ltx_p">In problems where the measures have an MDP structure similar to the objective, it may be feasible to estimate each <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><msub><mi>m</mi><mi>i</mi></msub></msub></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{m_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93621em;vertical-align:-.2501em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139199999999997em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3280857142857143em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span> with its own TD3 instance. In the environments in our work (Sec. <a href="#S5.SS1" title="5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>), each measure is non-Markovian since it calculates the proportion of time a walking agent’s foot spends on the ground. This calculation depends on the entire agent trajectory rather than on one state.</p></div></section></section><section id="S4.SS2" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>CMA-MEGA Variants</h3><figure id="algorithm1" class="mx-auto max-w-screen-md"><img class="w-full b-lazy my-4" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="Algorithm 1" data-src="static/algorithms/1.svg"></figure><div id="S4.SS2.p1" class="ltx_para"><p class="ltx_p">Our choice of gradient approximations leads to two CMA-MEGA variants. <span class="ltx_text ltx_font_bold">CMA</span><span class="ltx_text ltx_font_bold">-MEGA (ES)</span> approximates objective and measure gradients with OpenAI-ES, while <span class="ltx_text ltx_font_bold">CMA</span><span class="ltx_text ltx_font_bold">-MEGA (TD3, ES)</span> approximates the objective gradient with TD3 and the measure gradients with OpenAI-ES. Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an overview of both algorithms, and Algorithm <a href="#algorithm1" title="Algorithm 1 ‣ 4.2 CMA-MEGA Variants ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows their pseudocode. Since CMA-MEGA (TD3, ES) builds on CMA-MEGA (ES), we include only one algorithm and highlight lines that CMA-MEGA (TD3, ES) additionally executes. The rest of this section reviews the pseudocode in Algorithm <a href="#algorithm1" title="Algorithm 1 ‣ 4.2 CMA-MEGA Variants ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p></div><div id="S4.SS2.p2" class="ltx_para"><p class="ltx_p">We first set the batch size <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>λ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\lambda^{\prime}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.751892em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.751892em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> for CMA-ES (line 2). While CMA-MEGA (ES) and CMA-MEGA (TD3, ES) both have a total batch size of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span> solutions on each iteration, one of these solutions is reserved for evaluating <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.967476em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span> (line 7). In CMA-MEGA (TD3, ES), an additional solution is reserved for evaluating the greedy actor (line 26). This leaves <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>λ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\lambda^{\prime}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.751892em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.751892em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> solutions remaining for the CMA-ES instance.</p></div><div id="S4.SS2.p3" class="ltx_para"><p class="ltx_p">Next, we initialize various objects (lines 3-4). CMA-MEGA (TD3, ES) also initializes a replay buffer <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span></span></span></span>, critic networks <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msub><mi mathvariant="bold-italic">θ</mi><mn>1</mn></msub></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}_{1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93343em;vertical-align:-.2501em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31731428571428577em"><span style="top:-2.357em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msub><mi mathvariant="bold-italic">θ</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}_{2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93343em;vertical-align:-.2501em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31731428571428577em"><span style="top:-2.357em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span>, a “greedy” actor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><msub><mi mathvariant="bold-italic">ϕ</mi><mi>q</mi></msub></msub></mrow><annotation encoding="application/x-tex">\pi_{{\bm{\phi}}_{q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.863888em;vertical-align:-.43332799999999994em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.04167428571428572em"><span style="top:-2.2341314285714287em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.03588em">q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.40475428571428573em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.43332799999999994em"><span></span></span></span></span></span></span></span></span></span> which is used to train the critics, and target networks <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msubsup><mi mathvariant="bold-italic">θ</mi><mn>1</mn><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}^{\prime}_{1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.07573em;vertical-align:-.3924000000000001em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.52566em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7416285714285715em"><span style="top:-2.188485714285714em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span style="top:-2.8448em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.31151428571428574em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3924000000000001em"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msubsup><mi mathvariant="bold-italic">θ</mi><mn>2</mn><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}^{\prime}_{2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.07573em;vertical-align:-.3924000000000001em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.52566em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7416285714285715em"><span style="top:-2.188485714285714em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-2.8448em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.31151428571428574em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3924000000000001em"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><msubsup><mi mathvariant="bold-italic">ϕ</mi><mi>q</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></msub></mrow><annotation encoding="application/x-tex">\pi_{{\bm{\phi}}^{\prime}_{q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9000679999999999em;vertical-align:-.46950799999999987em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.527912em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7384114285714285em"><span style="top:-2.214em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.03588em">q</span></span></span></span><span style="top:-2.841582857142857em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.42488571428571426em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.46950799999999987em"><span></span></span></span></span></span></span></span></span></span> (line 5).</p></div><div id="S4.SS2.p4" class="ltx_para"><p class="ltx_p">In the main loop (line 6), we first evaluate <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.967476em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span> (line 7) and insert it into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal">A</span></span></span></span></span> (line 8). Then, we estimate the objective and measure gradients of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.967476em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span> with OpenAI-ES (line 9). This estimate computes all objective and measure gradients after evaluating <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mrow><mi>e</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\lambda_{es}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> solutions (Algorithm <a href="#algorithm4" title="Algorithm 4 ‣ Appendix A Helper Methods for CMA-MEGA Variants ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, Appendix <a href="#A1" title="Appendix A Helper Methods for CMA-MEGA Variants ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>). In CMA-MEGA (TD3, ES), we override the objective gradient estimate with the TD3 estimate (line 10). This estimate samples <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>p</mi><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{pg}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">p</span><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> transitions of experience from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span></span></span></span> and computes a gradient ascent step with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><msub><mi mathvariant="bold-italic">θ</mi><mn>1</mn></msub></msub></mrow><annotation encoding="application/x-tex">Q_{{\bm{\theta}}_{1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.93343em;vertical-align:-.2501em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol" style="margin-right:.03194em">θ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31731428571428577em"><span style="top:-2.357em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2501em"><span></span></span></span></span></span></span></span></span></span> (Algorithm <a href="#algorithm3" title="Algorithm 3 ‣ Appendix A Helper Methods for CMA-MEGA Variants ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, Appendix <a href="#A1" title="Appendix A Helper Methods for CMA-MEGA Variants ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>). To make this estimate more accurate, we sample many transitions (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>p</mi><mi>g</mi></mrow></msub><mo>=</mo><mn>65</mn><mo separator="true">,</mo><mn>536</mn></mrow><annotation encoding="application/x-tex">n_{pg}=65,536</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">p</span><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">6</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">5</span><span class="mord">3</span><span class="mord">6</span></span></span></span> instead of the default of 100 in TD3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="Addressing function approximation error in actor-critic methods" class="ltx_ref">18</a>]</cite>).</p></div><div id="S4.SS2.p5" class="ltx_para"><p class="ltx_p">Once the gradients are computed, we normalize them to be unit vectors (line 11) and generate <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>λ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\lambda^{\prime}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.751892em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.751892em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> solutions for insertion into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal">A</span></span></span></span></span>. Specifically, we sample gradient coefficients <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">c</mi></mrow><annotation encoding="application/x-tex">{\bm{c}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">c</span></span></span></span></span></span></span> (line 13), compute perturbation <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83611em;vertical-align:-.15em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> (line 14), and take a gradient step to obtain <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{\prime}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.083232em;vertical-align:-.247em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-2.4530000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span> (line 15). Finally, we evaluate <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{\prime}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.083232em;vertical-align:-.247em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.836232em"><span style="top:-2.4530000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span> (line 16) and store the improvement <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Δ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord">Δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> (Sec. <a href="#S3.SS2.SSS2" title="3.2.2 Covariance Matrix Adaptation MAP-Elites via a Gradient Arborescence (CMA-MEGA) ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>) from inserting it into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal">A</span></span></span></span></span> (line 17).</p></div><div id="S4.SS2.p6" class="ltx_para"><p class="ltx_p">On line 19, we create an <span class="ltx_text ltx_font_italic">improvement ranking</span> for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{c}}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.59444em;vertical-align:-.15em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">c</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">∇</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\nabla}}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83611em;vertical-align:-.15em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">∇</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> based on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Δ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord">Δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>. We update the CMA-ES parameters and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.967476em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span> based on the ranking (lines 20-21). If CMA-ES did not generate any solutions that were inserted into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal">A</span></span></span></span></span>, we reset CMA-ES and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">ϕ</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">{\bm{\phi}}^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.967476em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.773036em"><span style="top:-3.1473400000000002em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span> (lines 22-24).</p></div><div id="S4.SS2.p7" class="ltx_para"><p class="ltx_p">Finally, we update the TD3 instance in CMA-MEGA (TD3, ES). First, identically to PGA-MAP-Elites, we evaluate and insert the greedy actor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">{\bm{\phi}}_{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0746879999999999em;vertical-align:-.380248em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.057252000000000025em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.03588em">q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.380248em"><span></span></span></span></span></span></span></span></span></span> (lines 26-27). Then, we add experience to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span></span></span></span> (line 28), including experience from the OpenAI-ES gradient estimate (line 9), and train the critics (line 29). In practice, this training executes in parallel with the rest of the loop to reduce runtime.</p></div></section></section><section id="S5" class="ltx_section"><h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2><div id="S5.p1" class="ltx_para"><p class="ltx_p">We compare our two proposed CMA-MEGA variants (CMA-MEGA (ES), CMA-MEGA (TD3, ES)) with three baselines (PGA-MAP-Elites, ME-ES, MAP-Elites) in four locomotion tasks. We implement MAP-Elites as described in Sec. <a href="#S3.SS2.SSS1" title="3.2.1 MAP-Elites extensions for QD-RL ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>, and we select the explore-exploit variant for ME-ES since it has performed at least as well as both the explore variant and the exploit variant in several domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="Scaling map-elites to deep neuroevolution" class="ltx_ref">8</a>]</cite>.</p></div><section id="S5.SS1" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation Domains</h3><section id="S5.SS1.SSS1" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>QDGym</h4><div id="S5.SS1.SSS1.p1" class="ltx_para"><p class="ltx_p">We evaluate our algorithms in four locomotion environments from QDGym <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="QDgym" class="ltx_ref">37</a>]</cite>, a library built on PyBullet Gym <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="PyBullet, a python module for physics simulation for games, robotics and machine learning" class="ltx_ref">11</a>, <a href="#bib.bib38" title="PyBullet gymperium" class="ltx_ref">14</a>]</cite> and OpenAI Gym <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="OpenAI gym" class="ltx_ref">5</a>]</cite>. Table <a href="#S5.T1" title="Table 1 ‣ 5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> lists all environment details. In each environment, the QD algorithm outputs an archive of walking policies for a simulated agent. The agent is primarily rewarded for its forward speed. There are also reward shaping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="Policy invariance under reward transformations: theory and application to reward shaping" class="ltx_ref">35</a>]</cite> signals, such as a punishment for applying higher joint torques, intended to guide policy optimization. The measures compute the proportion of time (number of timesteps divided by total timesteps in an episode) that each of the agent’s feet contacts the ground.</p></div><div id="S5.SS1.SSS1.p2" class="ltx_para"><p class="ltx_p">QDGym is challenging because the objective in each environment does not “align” with the measures, in that finding policies with different measures (i.e. exploring the archive) does not necessarily lead to optimization of the objective. While it may be trivial to fill the archive with low-performing policies which stand in place and lift the feet up and down to achieve different measures, the agents’ complexity (high degrees of freedom) makes it difficult to learn a high-performing policy for each value of the measures.</p></div><figure id="S5.T1" class="ltx_table mount_envs"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>QDGym environments details. We list the dimensions of the state space (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">S</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|\mathcal{S}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:.075em">S</span></span><span class="mord">∣</span></span></span></span>) and action space (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">U</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|\mathcal{U}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:.09931em">U</span></span><span class="mord">∣</span></span></span></span>), number of neural network parameters, number of measures <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">X</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|\mathcal{X}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:.14643em">X</span></span><span class="mord">∣</span></span></span></span>, archive grid dimensions (number of cells along each dimension), total archive grid cells, and min and max objectives (Sec. <a href="#S5.SS1.SSS3" title="5.1.3 Metrics ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>). See Appendix <a href="#A3" title="Appendix C Environment Measures ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> for more info on the measures.</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><tbody class="ltx_tbody"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_border_tt ltx_th_row ltx_align_left"><br class="ltx_break"></th><td class="ltx_td ltx_border_tt font-bold text-center">QD Ant</td><td class="ltx_td ltx_border_tt font-bold text-center">QD Half-Cheetah</td><td class="ltx_td ltx_border_tt font-bold text-center">QD Hopper</td><td class="ltx_td ltx_border_tt font-bold text-center">QD Walker</td></tr><tr class="ltx_tr py-3"><th class="ltx_td ltx_th ltx_th_row ltx_align_left"><br class="ltx_break"></th><td class="ltx_td"><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" id="S5.T1.g1" class="b-lazy mx-auto" width="65" height="54" alt="QD Ant" data-src="static/qdgym_env_imgs/ant.png"></td><td class="ltx_td"><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" id="S5.T1.g2" class="b-lazy mx-auto" width="65" height="49" alt="QD Half-Cheetah" data-src="static/qdgym_env_imgs/half_cheetah.png"></td><td class="ltx_td"><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" id="S5.T1.g3" class="b-lazy mx-auto" width="19" height="59" alt="QD Hopper" data-src="static/qdgym_env_imgs/hopper.png"></td><td class="ltx_td"><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" id="S5.T1.g4" class="b-lazy mx-auto" width="27" height="61" alt="QD Walker" data-src="static/qdgym_env_imgs/walker.png"></td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_t"><br class="ltx_break"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">S</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|\mathcal{S}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:.075em">S</span></span><span class="mord">∣</span></span></span></span></th><td class="ltx_td ltx_border_t ltx_align_right">28</td><td class="ltx_td ltx_border_t ltx_align_right">26</td><td class="ltx_td ltx_border_t ltx_align_right">15</td><td class="ltx_td ltx_border_t ltx_align_right">22</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">U</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|\mathcal{U}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:.09931em">U</span></span><span class="mord">∣</span></span></span></span></th><td class="ltx_td ltx_align_right">8</td><td class="ltx_td ltx_align_right">6</td><td class="ltx_td ltx_align_right">3</td><td class="ltx_td ltx_align_right">6</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left">Parameters</th><td class="ltx_td ltx_align_right">21,256</td><td class="ltx_td ltx_align_right">20,742</td><td class="ltx_td ltx_align_right">18,947</td><td class="ltx_td ltx_align_right">20,230</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">X</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|\mathcal{X}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:.14643em">X</span></span><span class="mord">∣</span></span></span></span></th><td class="ltx_td ltx_align_right">4</td><td class="ltx_td ltx_align_right">2</td><td class="ltx_td ltx_align_right">1</td><td class="ltx_td ltx_align_right">2</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left">Archive dim</th><td class="ltx_td ltx_align_right">[6,6,6,6]</td><td class="ltx_td ltx_align_right">[32,32]</td><td class="ltx_td ltx_align_right">[1024]</td><td class="ltx_td ltx_align_right">[32,32]</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left">Grid cells</th><td class="ltx_td ltx_align_right">1,296</td><td class="ltx_td ltx_align_right">1,024</td><td class="ltx_td ltx_align_right">1,024</td><td class="ltx_td ltx_align_right">1,024</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left">Min objective</th><td class="ltx_td ltx_align_right">-374.70</td><td class="ltx_td ltx_align_right">-2,797.52</td><td class="ltx_td ltx_align_right">-362.09</td><td class="ltx_td ltx_align_right">-67.17</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_bb">Max objective</th><td class="ltx_td ltx_border_bb ltx_align_right">2,500.00</td><td class="ltx_td ltx_border_bb ltx_align_right">3,000.00</td><td class="ltx_td ltx_border_bb ltx_align_right">2,500.00</td><td class="ltx_td ltx_border_bb ltx_align_right">2,500.00</td></tr></tbody></table></figure></section><section id="S5.SS1.SSS2" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Hyperparameters</h4><div id="S5.SS1.SSS2.p1" class="ltx_para"><p class="ltx_p">Each agent’s policy is a neural network which takes in states and outputs actions. There are two hidden layers of 128 nodes, and the hidden and output layers have <span class="ltx_text ltx_font_typewriter">tanh</span> activation. We initialize weights with Xavier initialization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="Understanding the difficulty of training deep feedforward neural networks" class="ltx_ref">19</a>]</cite>.</p></div><div id="S5.SS1.SSS2.p2" class="ltx_para"><p class="ltx_p">For the archive, we tesselate each environment’s measure space into a grid of evenly-sized cells (see Table <a href="#S5.T1" title="Table 1 ‣ 5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for grid dimensions). Each measure is bound to the range <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>, the min and max proportion of time that one foot can contact the ground.</p></div><div id="S5.SS1.SSS2.p3" class="ltx_para"><p class="ltx_p">Each algorithm evaluates 1 million solutions in the environment. Due to computational limits, we evaluate each solution once instead of averaging multiple episodes, so each algorithm runs 1 million episodes total. Refer to Appendix <a href="#A2" title="Appendix B Algorithm Hyperparameters ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for further hyperparameters.</p></div></section><section id="S5.SS1.SSS3" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Metrics</h4><div id="S5.SS1.SSS3.p1" class="ltx_para"><p class="ltx_p">Our primary metric is <span class="ltx_text ltx_font_italic">QD score</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="Quality diversity: a new frontier for evolutionary computation" class="ltx_ref">40</a>]</cite>, which provides a holistic view of algorithm performance. QD score is the sum of the objective values of all elites in the archive, i.e. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><msub><mi mathvariant="bold">1</mi><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi></msub><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi></mrow></mrow></msub><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sum_{i=1}^{M}\bm{1}_{{\bm{\phi}}_{i}\mathrm{exists}}f({\bm{\phi}}_{i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.317339em;vertical-align:-.33610799999999996em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-.0000050000000000050004em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.981231em"><span style="top:-2.40029em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span><span class="mtight mrel">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.29971000000000003em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">1</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20521714285714282em"><span style="top:-2.2341314285714287em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.26586857142857145em"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mtight mathrm">e</span><span class="mord mtight mathrm">x</span><span class="mord mtight mathrm">i</span><span class="mord mtight mathrm">s</span><span class="mord mtight mathrm">t</span><span class="mord mtight mathrm">s</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.21752399999999997em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span> is the number of archive cells. We set the objective <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> to be the <span class="ltx_text ltx_font_italic">expected undiscounted return</span>, i.e. we set <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.05556em">γ</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> in Eq. <a href="#S2.E2" title="(2) ‣ 2.2 Quality Diversity for Reinforcement Learning (QD-RL) ‣ 2 Problem Statement ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p></div><div id="S5.SS1.SSS3.p2" class="ltx_para"><p class="ltx_p">Since objectives may be negative, an algorithm’s QD score may be penalized when adding a new solution. To prevent this, we define a <span class="ltx_text ltx_font_italic">minimum objective</span> in each environment by taking the lowest objective value that was inserted into the archive in any experiment in that environment. We subtract this minimum from every solution, such that every solution that was inserted into an archive has an objective value of at least 0. Thus, we use QD score defined as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><msub><mi mathvariant="bold">1</mi><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi></msub><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi></mrow></mrow></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">ϕ</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mtext>&nbsp;</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">j</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sum_{i=1}^{M}\bm{1}_{{\bm{\phi}}_{i}\mathrm{exists}}(f({\bm{\phi}}_{i})-% \mathrm{min\ objective})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.317339em;vertical-align:-.33610799999999996em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-.0000050000000000050004em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.981231em"><span style="top:-2.40029em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span><span class="mtight mrel">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.29971000000000003em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">1</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.20521714285714282em"><span style="top:-2.2341314285714287em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.26586857142857145em"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mtight mathrm">e</span><span class="mord mtight mathrm">x</span><span class="mord mtight mathrm">i</span><span class="mord mtight mathrm">s</span><span class="mord mtight mathrm">t</span><span class="mord mtight mathrm">s</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">ϕ</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.21752399999999997em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">m</span><span class="mord mathrm">i</span><span class="mord mathrm">n</span><span class="mspace">&nbsp;</span><span class="mord mathrm">o</span><span class="mord mathrm">b</span><span class="mord mathrm">j</span><span class="mord mathrm">e</span><span class="mord mathrm">c</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm" style="margin-right:.01389em">v</span><span class="mord mathrm">e</span></span><span class="mclose">)</span></span></span></span>. We also define a <span class="ltx_text ltx_font_italic">maximum objective</span> equivalent to each environment’s “reward threshold” in PyBullet Gym. This threshold is the objective value at which an agent is considered to have successfully learned to walk.</p></div><div id="S5.SS1.SSS3.p3" class="ltx_para"><p class="ltx_p">We report two metrics in addition to QD score. <span class="ltx_text ltx_font_italic">Archive coverage</span>, the proportion of cells for which the algorithm found an elite, gauges how well the QD algorithm explores measure space, and <span class="ltx_text ltx_font_italic">best performance</span>, the highest objective of any elite in the archive, gauges how well the QD algorithm exploits the objective.</p></div></section></section><section id="S5.SS2" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Experimental Design</h3><div id="S5.SS2.p1" class="ltx_para"><p class="ltx_p">We follow a between-groups design, where the two independent variables are environment (QD Ant, QD Half-Cheetah, QD Hopper, QD Walker) and algorithm (CMA-MEGA (ES), CMA-MEGA (TD3, ES), PGA-MAP-Elites, ME-ES, MAP-Elites). The dependent variable is the QD score. In each environment, we run each algorithm for 5 trials with different random seeds. We test three hypotheses:</p><ul id="S5.I1" class="list-disc ltx_itemize"><li id="S5.I1.i1" class="ltx_item"><div id="S5.I1.i1.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">H1: </span>In each environment, CMA-MEGA (ES) will outperform (with respect to QD score) all baselines (PGA-MAP-Elites, ME-ES, MAP-Elites).</p></div></li><li id="S5.I1.i2" class="ltx_item"><div id="S5.I1.i2.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">H2: </span>In each environment, CMA-MEGA (TD3, ES) will outperform all baselines.</p></div></li><li id="S5.I1.i3" class="ltx_item"><div id="S5.I1.i3.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">H3: </span>In each environment, CMA-MEGA (TD3, ES) will outperform CMA-MEGA (ES).</p></div></li></ul><p class="ltx_p">H1 and H2 are based on prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="Differentiable quality diversity" class="ltx_ref">16</a>]</cite> which showed that in QD benchmark domains, CMA-MEGA outperforms algorithms that do not leverage both objective and measure gradients. H3 is based on results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="Efficacy of modern neuro-evolutionary strategies for continuous control optimization" class="ltx_ref">38</a>]</cite> which suggest that actor-critic methods outperform ES in PyBullet Gym. Thus, we expect the TD3 objective gradient to be more accurate than the ES objective gradient, leading to more efficient traversal of objective-measure space and higher QD score.</p></div></section><section id="S5.SS3" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.3 </span>Implementation</h3><div id="S5.SS3.p1" class="ltx_para"><p class="ltx_p">We implement all QD algorithms with the pyribs library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="Pyribs: a bare-bones python library for quality diversity optimization" class="ltx_ref">47</a>]</cite> except for ME-ES, which we adapt from the authors’ implementation. We run each experiment with 100 CPUs on a high-performance cluster. We allocate one NVIDIA Tesla P100 GPU to algorithms that train TD3 (CMA-MEGA (TD3, ES) and PGA-MAP-Elites). Depending on the algorithm and environment, each experiment lasts 4-20 hours; refer to Table <a href="#A4.T12" title="Table 12 ‣ Appendix D Final Metrics ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, Appendix <a href="#A4" title="Appendix D Final Metrics ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> for mean runtimes. We have released our source code at <a href="https://github.com/icaros-usc/dqd-rl" title="" class="ltx_ref ltx_font_typewriter ltx_url">https://github.com/icaros-usc/dqd-rl</a></p></div></section></section><section id="S6" class="ltx_section"><h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Results</h2><figure id="S6.F3" class="mx-auto ltx_figure max-w-screen-xl mount_comparison"><img class="w-full b-lazy" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="Plots of metrics for all algorithms in all environments. Refer to caption." data-src="static/comparison.svg"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Plots of QD score, archive coverage, and best performance for the 5 algorithms in our experiments in all 4 environments from QDGym. The x-axis in all plots is the number of solutions evaluated. Solid lines show the mean over 5 trials, and shaded regions show the standard error of the mean.</figcaption></figure><div id="S6.p1" class="ltx_para"><p class="ltx_p">We ran 5 trials of each algorithm in each environment. In each trial, we allocated 1 million evaluations and recorded the QD score, archive coverage, and best performance. Fig. <a href="#S6.F3" title="Figure 3 ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> plots these metrics, and Appendix <a href="#A4" title="Appendix D Final Metrics ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> lists final values of all metrics. Appendix <a href="#A6" title="Appendix F Archive Visualizations ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a> shows example heatmaps and histograms of each archive, and the supplemental material contains videos of generated agents.</p></div><section id="S6.SS1" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.1 </span>Analysis</h3><div id="S6.SS1.p1" class="ltx_para"><p class="ltx_p">To test our hypotheses, we conducted a two-way ANOVA which examined the effect of algorithm and environment on the QD score. We note that the ANOVA requires QD scores to have the same scale, but each environment’s QD score has a different scale by default. Thus, for this analysis, we normalized QD scores by dividing by each environment’s maximum QD score, defined as <span class="ltx_text ltx_font_italic">grid cells</span> * (<span class="ltx_text ltx_font_italic">max objective</span> - <span class="ltx_text ltx_font_italic">min objective</span>) (see Table <a href="#S5.T1" title="Table 1 ‣ 5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for these quantities).</p></div><div id="S6.SS1.p2" class="ltx_para"><p class="ltx_p">We found a statistically significant interaction between algorithm and environment on QD score, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>12</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>16.82</mn><mo separator="true">,</mo><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">F(12,80)=16.82,p&lt;0.001</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">1</span><span class="mord">6</span><span class="mord">.</span><span class="mord">8</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">0</span><span class="mord">1</span></span></span></span>. Simple main effects analysis indicated that the algorithm had a significant effect on QD score in each environment, so we ran pairwise comparisons (two-sided t-tests) with Bonferroni corrections (Appendix <a href="#A5" title="Appendix E Full Statistical Analysis ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>). Our results are as follows:</p></div><div id="S6.SS1.p3" class="ltx_para"><ul id="S6.I1" class="list-disc ltx_itemize"><li id="S6.I1.i1" class="ltx_item"><div id="S6.I1.i1.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">H1:</span> There is no significant difference in QD score between CMA-MEGA (ES) and PGA-MAP-Elites in QD Ant and QD Half-Cheetah, but in QD Hopper and QD Walker, CMA-MEGA (ES) attains significantly lower QD score than PGA-MAP-Elites. CMA-MEGA (ES) achieves significantly higher QD score than ME-ES in all environments except QD Hopper, where there is no significant difference. There is no significant difference between CMA-MEGA (ES) and MAP-Elites in all domains except QD Hopper, where CMA-MEGA (ES) attains significantly lower QD score.</p></div></li><li id="S6.I1.i2" class="ltx_item"><div id="S6.I1.i2.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">H2:</span> In all environments, there is no significant difference in QD score between CMA-MEGA (TD3, ES) and PGA-MAP-Elites. CMA-MEGA (TD3, ES) achieves significantly higher QD score than ME-ES in all environments. CMA-MEGA (TD3, ES) achieves significantly higher QD score than MAP-Elites in QD Half-Cheetah and QD Walker, but there is no significant difference in QD Ant and QD Hopper.</p></div></li><li id="S6.I1.i3" class="ltx_item"><div id="S6.I1.i3.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">H3:</span> CMA-MEGA (TD3, ES) achieves significantly higher QD score than CMA-MEGA (ES) in QD Hopper and QD Walker, but there is no significant difference in QD Ant and QD Half-Cheetah.</p></div></li></ul></div></section><section id="S6.SS2" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.2 </span>Discussion</h3><div id="S6.SS2.p1" class="ltx_para"><p class="ltx_p">We discuss how the CMA-MEGA variants differ from the baselines (Sec. <a href="#S6.SS2.SSS1" title="6.2.1 PGA-MAP-Elites and objective-measure space exploration ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.1</span></a>-<a href="#S6.SS2.SSS4" title="6.2.4 MAP-Elites and robustness ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.4</span></a>) and how they differ from each other (Sec. <a href="#S6.SS2.SSS5" title="6.2.5 CMA-MEGA variants and gradient estimates ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.5</span></a>).</p></div><section id="S6.SS2.SSS1" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>PGA-MAP-Elites and objective-measure space exploration</h4><div id="S6.SS2.SSS1.p1" class="ltx_para"><p class="ltx_p">Of the CMA-MEGA variants, CMA-MEGA (TD3, ES) performed the closest to PGA-MAP-Elites, with no significant QD score difference in any environment. This result differs from prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="Differentiable quality diversity" class="ltx_ref">16</a>]</cite> in QD benchmark domains, where CMA-MEGA outperformed OG-MAP-Elites, a baseline DQD algorithm inspired by PGA-MAP-Elites.</p></div><div id="S6.SS2.SSS1.p2" class="ltx_para"><p class="ltx_p">We attribute this difference to the difficulty of exploring objective-measure space in the benchmark domains. For example, the linear projection benchmark domain is designed to be “distorted” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="Covariance matrix adaptation for the rapid illumination of behavior space" class="ltx_ref">17</a>]</cite>. Values in the center of its measure space are easy to obtain with random sampling, while values at the edges are unlikely to be sampled. Hence, high QD score arises from traversing measure space and filling the archive. In contrast, as discussed in Sec. <a href="#S5.SS1.SSS1" title="5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>, it is relatively easy to fill the archive in QDGym. We see this empirically — in all environments, all algorithms achieve nearly 100% archive coverage. This may even happen within the first 100k evaluations, as in QD Half-Cheetah (Fig. <a href="#S6.F3" title="Figure 3 ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Hence, high QD score in QDGym comes from optimizing solutions after filling the archive.</p></div><div id="S6.SS2.SSS1.p3" class="ltx_para"><p class="ltx_p">Since CMA-MEGA adapts its sampling distribution, it performs the exploration necessary to succeed in the linear projection domain, while OG-MAP-Elites remains “stuck” in the center of the measure space. However, this exploration capability does not increase QD score in QDGym. Instead, QDGym is more appropriate for an algorithm which rigorously optimizes the objective. PGA-MAP-Elites does exactly this: every iteration, it increases the objective value of half of its generated solutions by optimizing them with respect to a TD3 critic. Though CMA-MEGA (TD3, ES) also trains a TD3 critic, it does not perform this additional optimization. Naturally, this leads to a possible extension in which solutions sampled by CMA-MEGA (TD3, ES) are optimized with respect to its TD3 critic before being evaluated in the environment.</p></div></section><section id="S6.SS2.SSS2" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>PGA-MAP-Elites and optimization efficiency</h4><div id="S6.SS2.SSS2.p1" class="ltx_para"><p class="ltx_p">While there was no significant difference in the <span class="ltx_text ltx_font_italic">final</span> QD scores of CMA-MEGA (TD3, ES) and PGA-MAP-Elites, CMA-MEGA (TD3, ES) was less <span class="ltx_text ltx_font_italic">efficient</span> than PGA-MAP-Elites in some environments. For instance, in QD Hopper, PGA-MAP-Elites reached 1.5M QD score after 100k evaluations, but CMA-MEGA (TD3, ES) required 400k evaluations.</p></div><div id="S6.SS2.SSS2.p2" class="ltx_para"><p class="ltx_p">We can quantify optimization efficiency with <span class="ltx_text ltx_font_italic">QD score AUC</span>, the area under the curve (AUC) of the QD score plot. For a QD algorithm which executes <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span> iterations and evaluates <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span> solutions per iteration, we define QD score AUC as a Riemann sum: QD score AUC <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo stretchy="false">(</mo><mi>λ</mi><mo>∗</mo></mrow><annotation encoding="application/x-tex">=\sum_{i=1}^{N}(\lambda*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.36687em;vertical-align:0"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.2809409999999999em;vertical-align:-.29971000000000003em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-.0000050000000000050004em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.981231em"><span style="top:-2.40029em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">i</span><span class="mtight mrel">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.29971000000000003em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">λ</span><span class="mord">∗</span></span></span></span> QD score at iteration <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span> After computing QD score AUC, we ran statistical analysis similar to Sec. <a href="#S6.SS1" title="6.1 Analysis ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a> and found CMA-MEGA (TD3, ES) had significantly lower QD score AUC than PGA-MAP-Elites in QD Ant and QD Hopper. There was no significant difference in QD Half-Cheetah and QD Walker. As such, while CMA-MEGA (TD3, ES) obtained comparable final QD scores to PGA-MAP-Elites in all tasks, it was less efficient at achieving those scores in QD Ant and QD Hopper.</p></div></section><section id="S6.SS2.SSS3" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">6.2.3 </span>ME-ES and archive insertions</h4><div id="S6.SS2.SSS3.p1" class="ltx_para"><p class="ltx_p">With one exception (CMA-MEGA (ES) in QD Hopper), both CMA-MEGA variants achieved significantly higher QD score than ME-ES in all environments. We attribute this result to the number of solutions each algorithm inserts into the archive. Each iteration, ME-ES evaluates 200 solutions (Appendix <a href="#A2" title="Appendix B Algorithm Hyperparameters ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>) but only inserts one into the archive, for a total of 5000 solutions inserted during each run. Given that each archive has at least 1000 cells, ME-ES has, on average, 5 opportunities to insert a solution that improves each cell. In contrast, the CMA-MEGA variants have 100 times more insertions. Though the variants also evaluate 200 solutions per iteration, they insert 100 of these into the archive. This totals to 500k insertions per run, allowing the variants to gradually improve archive cells.</p></div></section><section id="S6.SS2.SSS4" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">6.2.4 </span>MAP-Elites and robustness</h4><div id="S6.SS2.SSS4.p1" class="ltx_para"><p class="ltx_p">In most cases, both CMA-MEGA variants had significantly higher QD score than MAP-Elites or no significant difference, but in QD Hopper, MAP-Elites achieved significantly higher QD score than CMA-MEGA (ES). However, when we visualized solutions found by MAP-Elites, their performance was lower than the performance recorded in the archive. The best MAP-Elites solution in QD Hopper hopped forward a few steps and fell down, despite recording an excellent performance of 2,648.31.</p></div><div id="S6.SS2.SSS4.p2" class="ltx_para"><p class="ltx_p">One explanation for this behavior is that since we only evaluate solutions for one episode before inserting into the archive, a solution with noisy performance may be inserted because of a single high-performing episode, even if it performs poorly on average. Prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="Policy gradient assisted map-elites" class="ltx_ref">36</a>]</cite> has also encountered this issue when running MAP-Elites with a directional variation operator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="Discovering the elite hypervolume by leveraging interspecies correlation" class="ltx_ref">48</a>]</cite> in QDGym, and has suggested measuring <span class="ltx_text ltx_font_italic">robustness</span> as a proxy for how much noise is present in an archive’s solutions. Robustness is defined as the difference between the mean performance of the solution over <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span> episodes (we use <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">n=10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">0</span></span></span></span>) and the performance recorded in the archive. The larger (more negative) this difference, the more noisy and less robust the solution.</p></div><div id="S6.SS2.SSS4.p3" class="ltx_para"><p class="ltx_p">To compare the robustness of the solutions output by the CMA-MEGA variants and MAP-Elites, we computed <span class="ltx_text ltx_font_italic">mean elite robustness</span>, the average robustness of all elites in each experiment’s final archive. We then ran statistical analysis similar to Sec. <a href="#S6.SS1" title="6.1 Analysis ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>. In all environments, both CMA-MEGA (ES) and CMA-MEGA (TD3, ES) had significantly higher mean elite robustness than MAP-Elites (Appendix <a href="#A4" title="Appendix D Final Metrics ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> &amp; <a href="#A5" title="Appendix E Full Statistical Analysis ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>). Overall, though MAP-Elites achieves high QD score, its solutions are less robust.</p></div></section><section id="S6.SS2.SSS5" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">6.2.5 </span>CMA-MEGA variants and gradient estimates</h4><div id="S6.SS2.SSS5.p1" class="ltx_para"><p class="ltx_p">In QD Hopper and QD Walker, CMA-MEGA (TD3, ES) had significantly higher QD score than CMA-MEGA (ES). One potential explanation is that PyBullet Gym (and hence QDGym) augments rewards with reward shaping signals intended to promote optimal solutions for deep RL algorithms. In prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="Efficacy of modern neuro-evolutionary strategies for continuous control optimization" class="ltx_ref">38</a>]</cite>, these signals led PPO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="Proximal policy optimization algorithms" class="ltx_ref">44</a>]</cite> to train successful walking agents, while they led OpenAI-ES into local optima. For instance, OpenAI-ES trained agents which stood still so as to maximize only the reward signal for staying upright.</p></div><div id="S6.SS2.SSS5.p2" class="ltx_para"><p class="ltx_p">Due to these signals, TD3’s objective gradient seems more useful than that of OpenAI-ES in QD Hopper and QD Walker. In fact, the algorithms which performed best in QD Hopper and QD Walker were ones that calculated objective gradients with TD3, i.e. PGA-MAP-Elites and CMA-MEGA (TD3, ES).</p></div><div id="S6.SS2.SSS5.p3" class="ltx_para"><p class="ltx_p">Prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="Efficacy of modern neuro-evolutionary strategies for continuous control optimization" class="ltx_ref">38</a>]</cite> found that rewards could be tailored for ES, such that OpenAI-ES outperformed PPO. Extensions of our work could investigate whether there is a similar effect for QD algorithms, where tailoring the reward leads CMA-MEGA (ES) to outperform PGA-MAP-Elites and CMA-MEGA (TD3, ES).</p></div></section><section id="S6.SS2.SSS6" class="ltx_subsubsection"><h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">6.2.6 </span>Computational effort</h4><div id="S6.SS2.SSS6.p1" class="ltx_para"><p class="ltx_p">Though CMA-MEGA (TD3, ES) and PGA-MAP-Elites perform best overall, they rely on specialized hardware (a GPU) and require the most computation. As shown in Table <a href="#A4.T12" title="Table 12 ‣ Appendix D Final Metrics ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, Appendix <a href="#A4" title="Appendix D Final Metrics ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>, the TD3 training in these algorithms leads to long runtimes. When runtime is dominated by the algorithm itself (as opposed to solution evaluations), CMA-MEGA (ES) offers a viable alternative that may achieve reasonable performance.</p></div></section></section></section><section id="S7" class="ltx_section"><h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><div id="S7.p1" class="ltx_para"><p class="ltx_p">To extend DQD to RL settings, we adapted gradient approximations from actor-critic methods and ES. By integrating these approximations with CMA-MEGA, we proposed two novel variants that we evaluated on four locomotion tasks from QDGym. CMA-MEGA (TD3, ES) performed comparably to the state-of-the-art PGA-MAP-Elites in all tasks but was less efficient in two of the tasks. CMA-MEGA (ES) performed comparably in two tasks.</p></div><div id="S7.p2" class="ltx_para"><p class="ltx_p">Our results contrast prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="Differentiable quality diversity" class="ltx_ref">16</a>]</cite> where CMA-MEGA outperformed a baseline algorithm inspired by PGA-MAP-Elites in QD benchmark domains. The difference seems to be that difficulty in the benchmarks arises from a hard-to-explore measure space, whereas difficulty in QDGym arises from an objective which requires rigorous optimization. As such, future work could formalize the notions of “exploration difficulty” of a measure space and “optimization difficulty” of an objective and evaluate algorithms in benchmarks that cover a spectrum of these metrics.</p></div><div id="S7.p3" class="ltx_para"><p class="ltx_p">For practitioners looking to apply DQD in RL settings, we recommend estimating objective gradients with an off-policy actor-critic method such as TD3 instead of with an ES. Due to the difficulty of modern control benchmarks, it is important to efficiently optimize the objective — TD3 benefits over ES since it can compute the objective gradient without further environment interaction. Furthermore, reward signals in these benchmarks are designed for deep RL methods, making TD3 gradients more useful than ES gradients.</p></div><div id="S7.p4" class="ltx_para"><p class="ltx_p">Since we decouple DQD from gradient approximations, our work opens avenues for future research which expands the applicability of DQD in RL settings, by independently developing either more accurate gradient approximations or more powerful DQD algorithms.</p></div><div class="ltx_para"><p class="ltx_p italic">If you have any questions or comments, please visit the <a href="https://github.com/dqd-rl/dqd-rl.github.io/discussions">GitHub discussions</a> for this page.</p></div></section></article><article class="ltx_document ltx2_supplemental"><section id="Sx1" class="ltx_section ltx2_acknowledgements"><h2 class="ltx_title ltx_title_section">Acknowledgments</h2><div id="Sx1.p1" class="ltx_para"><p class="ltx_p">The authors thank Ya-Chuan Hsu, Heramb Nemlekar, and Gautam Salhotra for their valuable feedback.</p></div><div id="Sx1.p2" class="ltx_para"><p class="ltx_p">This website was built with <a href="https://www.11ty.dev">11ty</a>, <a href="https://cheerio.js.org">cheerio</a>, <a href="https://katex.org/">KaTeX</a>, <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML</a>, <a href="https://preactjs.com">Preact</a>, and <a href="https://tailwindcss.com">tailwindcss</a>.</p></div></section><section class="ltx_appendix" id="citation"><h2 class="ltx_title ltx_title_section">Citation</h2><div class="ltx_para"><p class="ltx_p">This work is currently a <a href="https://arxiv.org/abs/2202.03666">preprint on arXiv</a>.</p></div><div class="ltx_para mt-4"><div class="relative"><div id="clipboard-citation" class="text-sm bg-gray-50 bg-white overflow-x-auto p-4 rounded-lg"><pre><code>@misc{tjanaka2022approximating,
  title         = {Approximating Gradients for Differentiable Quality Diversity
                   in Reinforcement Learning},
  author        = {Bryon Tjanaka and Matthew C. Fontaine and Julian Togelius
                   and Stefanos Nikolaidis},
  year          = {2022},
  eprint        = {2202.03666},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  url           = {https://dqd-rl.github.io},
  note          = "\url{https://dqd-rl.github.io}",
}</code></pre></div><button class="flex items-center rounded-lg transition absolute clipboard cursor-pointer focus:bg-gray-900 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-900 hover:bg-opacity-50 hover:text-white p-2 right-0 text-gray-500 text-xs top-0" data-clipboard-target="#clipboard-citation"><span class="pr-2 hidden group-focus:hidden group-hover:inline-block">Copy </span><span class="pr-2 hidden group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></section><section class="ltx_appendix" id="open-source-code"><h2 class="ltx_title ltx_title_section">Open Source Code</h2><div class="ltx_para"><p class="ltx_p">The code for our experiments is available at <a href="https://github.com/icaros-usc/dqd-rl" title="" class="ltx_ref ltx_font_typewriter ltx_url">https://github.com/icaros-usc/dqd-rl</a></p><p></p></div></section><section class="ltx_appendix" id="license"><h2 class="ltx_title ltx_title_section">License</h2><div class="ltx_para"><p class="ltx_p">The text and figures of this work are licensed under the Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> unless otherwise noted.</p></div></section><section id="A1" class="ltx_appendix"><h2 class="ltx_title ltx_title_appendix"><span class="ltx_tag ltx_tag_appendix">Appendix A </span>Helper Methods for CMA-MEGA Variants</h2><figure id="algorithm2" class="mx-auto max-w-screen-md"><img class="w-full b-lazy my-4" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="Algorithm 2" data-src="static/algorithms/2.svg"></figure><figure id="algorithm3" class="mx-auto max-w-screen-md"><img class="w-full b-lazy my-4" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="Algorithm 3" data-src="static/algorithms/3.svg"></figure><figure id="algorithm4" class="mx-auto max-w-screen-md"><img class="w-full b-lazy my-4" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="Algorithm 4" data-src="static/algorithms/4.svg"></figure></section><section id="A2" class="ltx_appendix"><h2 class="ltx_title ltx_title_appendix"><span class="ltx_tag ltx_tag_appendix">Appendix B </span>Algorithm Hyperparameters</h2><div id="A2.p1" class="ltx_para"><p class="ltx_p">Here we list parameters for each algorithm in our experiments. Refer to Sec. <a href="#S5.SS1.SSS2" title="5.1.2 Hyperparameters ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.2</span></a> for parameters of the neural network policy and the archive. All algorithms are allocated 1,000,000 evaluations total.</p></div><figure id="A2.T2" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>CMA-MEGA (ES) and CMA-MEGA (TD3, ES) hyperparameters. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>p</mi><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{pg}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">p</span><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>c</mi><mi>r</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{crit}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">c</span><span class="mord mtight mathnormal" style="margin-right:.02778em">r</span><span class="mord mtight mathnormal">i</span><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> are only applicable in CMA-MEGA (TD3, ES). <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>p</mi><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{pg}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">p</span><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> here is analogous to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>p</mi><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{pg}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">p</span><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> in PGA-MAP-Elites, but it is much larger here since we only compute one gradient per iteration instead of taking gradient steps on multiple solutions.</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Parameter</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Description</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:right">Value</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td ltx_border_t" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span></td><td class="ltx_td ltx_border_t" style="text-align:left">Iterations = 1,000,000 / (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>+</mo><msub><mi>λ</mi><mrow><mi>e</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\lambda+\lambda_{es}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>)</td><td class="ltx_td ltx_border_t" style="text-align:right">5,000</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span></td><td class="ltx_td" style="text-align:left">Batch size</td><td class="ltx_td" style="text-align:right">100</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">\sigma_{g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">Initial <span class="ltx_text ltx_align_left">CMA</span>-<span class="ltx_text ltx_align_left">ES</span> step size</td><td class="ltx_td" style="text-align:right">1.0</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">η</span></span></span></span></td><td class="ltx_td" style="text-align:left">Gradient ascent learning rate</td><td class="ltx_td" style="text-align:right">1.0</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mrow><mi>e</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\lambda_{es}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">ES batch size</td><td class="ltx_td" style="text-align:right">100</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mi>e</mi></msub></mrow><annotation encoding="application/x-tex">\sigma_{e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">ES noise standard deviation</td><td class="ltx_td" style="text-align:right">0.02</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>p</mi><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{pg}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">p</span><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">TD3 gradient estimate batch size</td><td class="ltx_td" style="text-align:right">65,536</td></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>c</mi><mi>r</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{crit}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">c</span><span class="mord mtight mathnormal" style="margin-right:.02778em">r</span><span class="mord mtight mathnormal">i</span><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td ltx_border_bb" style="text-align:left">TD3 critic training steps</td><td class="ltx_td ltx_border_bb" style="text-align:right">600</td></tr></tbody></table></figure><figure id="A2.T3" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>PGA-MAP-Elites hyperparameters.</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Parameter</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Description</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:right">Value</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td ltx_border_t" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span></td><td class="ltx_td ltx_border_t" style="text-align:left">Iterations = 1,000,000 / <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span></td><td class="ltx_td ltx_border_t" style="text-align:right">10,000</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span></td><td class="ltx_td" style="text-align:left">Batch size</td><td class="ltx_td" style="text-align:right">100</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>e</mi><mi>v</mi><mi>o</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{evo}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal" style="margin-right:.03588em">v</span><span class="mord mtight mathnormal">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">Variation operators split</td><td class="ltx_td" style="text-align:right"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn><mi>λ</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">0.5\lambda=50</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">5</span><span class="mord">0</span></span></span></span></td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{grad}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span><span class="mord mtight mathnormal" style="margin-right:.02778em">r</span><span class="mord mtight mathnormal">a</span><span class="mord mtight mathnormal">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">PG variation steps</td><td class="ltx_td" style="text-align:right">10</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{grad}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:-.0037em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span><span class="mord mtight mathnormal" style="margin-right:.02778em">r</span><span class="mord mtight mathnormal">a</span><span class="mord mtight mathnormal">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">PG variation learning rate (for Adam)</td><td class="ltx_td" style="text-align:right">0.001</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>p</mi><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{pg}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">p</span><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">PG variation batch size</td><td class="ltx_td" style="text-align:right">256</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>c</mi><mi>r</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{crit}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">c</span><span class="mord mtight mathnormal" style="margin-right:.02778em">r</span><span class="mord mtight mathnormal">i</span><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">TD3 critic training steps</td><td class="ltx_td" style="text-align:right">300</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\sigma_{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">GA variation 1</td><td class="ltx_td" style="text-align:right">0.005</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\sigma_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">GA variation 2</td><td class="ltx_td" style="text-align:right">0.05</td></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal">G</span></span></span></span></td><td class="ltx_td ltx_border_bb" style="text-align:left">Random initial solutions</td><td class="ltx_td ltx_border_bb" style="text-align:right">100</td></tr></tbody></table></figure><figure id="A2.T4" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>ME-ES hyperparameters. We adopt the explore-exploit variant.</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Parameter</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Description</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:right">Value</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td ltx_border_t" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span></td><td class="ltx_td ltx_border_t" style="text-align:left">Iterations = 1,000,000 / <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span></td><td class="ltx_td ltx_border_t" style="text-align:right">5,000</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span></td><td class="ltx_td" style="text-align:left">Batch size</td><td class="ltx_td" style="text-align:right">200</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span></span></span></span></td><td class="ltx_td" style="text-align:left">ES noise standard deviation</td><td class="ltx_td" style="text-align:right">0.02</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>g</mi><mi>e</mi><mi>n</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{optim\_gens}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.79756em;vertical-align:-.367em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">o</span><span class="mord mtight mathnormal">p</span><span class="mord mtight mathnormal">t</span><span class="mord mtight mathnormal">i</span><span class="mord mtight mathnormal">m</span><span class="mord mtight" style="margin-right:.02778em">_</span><span class="mord mtight mathnormal" style="margin-right:.03588em">g</span><span class="mord mtight mathnormal">e</span><span class="mord mtight mathnormal">n</span><span class="mord mtight mathnormal">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.367em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">Consecutive generations to optimize a solution</td><td class="ltx_td" style="text-align:right">10</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.0037em">α</span></span></span></span></td><td class="ltx_td" style="text-align:left">Learning rate for Adam</td><td class="ltx_td" style="text-align:right">0.01</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\alpha_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.0037em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">L2 coefficient for Adam</td><td class="ltx_td" style="text-align:right">0.005</td></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span></td><td class="ltx_td ltx_border_bb" style="text-align:left">Nearest neighbors for novelty calculation</td><td class="ltx_td ltx_border_bb" style="text-align:right">10</td></tr></tbody></table></figure><figure id="A2.T5" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>MAP-Elites hyperparameters. We describe MAP-Elites in Sec. <a href="#S3.SS2.SSS1" title="3.2.1 MAP-Elites extensions for QD-RL ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Parameter</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Description</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:right">Value</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td ltx_border_t" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span></td><td class="ltx_td ltx_border_t" style="text-align:left">Iterations = 1,000,000 / <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span></td><td class="ltx_td ltx_border_t" style="text-align:right">10,000</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span></td><td class="ltx_td" style="text-align:left">Batch size</td><td class="ltx_td" style="text-align:right">100</td></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span></span></span></span></td><td class="ltx_td ltx_border_bb" style="text-align:left">Gaussian noise standard deviation</td><td class="ltx_td ltx_border_bb" style="text-align:right">0.02</td></tr></tbody></table></figure><figure id="A2.T6" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>TD3 hyperparameters common to CMA-MEGA (TD3, ES) and PGA-MAP-Elites, which both train a TD3 instance. Furthermore, though we record the objective with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.05556em">γ</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> (Sec. <a href="#S5.SS1.SSS3" title="5.1.3 Metrics ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>), TD3 still executes with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma&lt;1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7335400000000001em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.05556em">γ</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>.</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Parameter</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">Description</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:right">Value</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td ltx_border_t" style="text-align:left">—</td><td class="ltx_td ltx_border_t" style="text-align:left">Critic layer sizes</td><td class="ltx_td ltx_border_t" style="text-align:right"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>256</mn><mo separator="true">,</mo><mn>256</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[256,256,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>c</mi><mi>r</mi><mi>i</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{crit}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.0037em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">c</span><span class="mord mtight mathnormal" style="margin-right:.02778em">r</span><span class="mord mtight mathnormal">i</span><span class="mord mtight mathnormal">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">Critic learning rate (for Adam)</td><td class="ltx_td" style="text-align:right">3e-4</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">n_{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal" style="margin-right:.03588em">q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">Critic training batch size</td><td class="ltx_td" style="text-align:right">256</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">B</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|\mathcal{B}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span><span class="mord">∣</span></span></span></span></td><td class="ltx_td" style="text-align:left">Max replay buffer size</td><td class="ltx_td" style="text-align:right">1,000,000</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.05556em">γ</span></span></span></span></td><td class="ltx_td" style="text-align:left">Discount factor</td><td class="ltx_td" style="text-align:right">0.99</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.1132em">τ</span></span></span></span></td><td class="ltx_td" style="text-align:left">Target network update rate</td><td class="ltx_td" style="text-align:right">0.005</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">d</span></span></span></span></td><td class="ltx_td" style="text-align:left">Target network update frequency</td><td class="ltx_td" style="text-align:right">2</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">\sigma_{p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td" style="text-align:left">Smoothing noise standard deviation</td><td class="ltx_td" style="text-align:right">0.2</td></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb" style="text-align:left"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mrow><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{clip}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight mathnormal">c</span><span class="mord mtight mathnormal" style="margin-right:.01968em">l</span><span class="mord mtight mathnormal">i</span><span class="mord mtight mathnormal">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span></td><td class="ltx_td ltx_border_bb" style="text-align:left">Smoothing noise clip</td><td class="ltx_td ltx_border_bb" style="text-align:right">0.5</td></tr></tbody></table></figure></section><section id="A3" class="ltx_appendix"><h2 class="ltx_title ltx_title_appendix"><span class="ltx_tag ltx_tag_appendix">Appendix C </span>Environment Measures</h2><div id="A3.p1" class="ltx_para"><p class="ltx_p">The measures in QDGym are the proportions of time that each foot contacts the ground. In each environment, the feet are ordered as follows:</p></div><div id="A3.p2" class="ltx_para"><ul id="A3.I1" class="list-disc ltx_itemize"><li id="A3.I1.i1" class="ltx_item"><div id="A3.I1.i1.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">QD Ant:</span> front left foot, front right foot, back left foot, back right foot</p></div></li><li id="A3.I1.i2" class="ltx_item"><div id="A3.I1.i2.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">QD Half-Cheetah:</span> front foot, back foot</p></div></li><li id="A3.I1.i3" class="ltx_item"><div id="A3.I1.i3.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">QD Hopper:</span> single foot</p></div></li><li id="A3.I1.i4" class="ltx_item"><div id="A3.I1.i4.p1" class="ltx_para"><p class="ltx_p"><span class="ltx_text ltx_font_bold">QD Walker:</span> right foot, left foot</p></div></li></ul></div></section><section id="A4" class="ltx_appendix"><h2 class="ltx_title ltx_title_appendix"><span class="ltx_tag ltx_tag_appendix">Appendix D </span>Final Metrics</h2><div id="A4.p1" class="ltx_para"><p class="ltx_p">Tables <a href="#A4.T7" title="Table 7 ‣ Appendix D Final Metrics ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>-<a href="#A4.T12" title="Table 12 ‣ Appendix D Final Metrics ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> show the QD score (Sec. <a href="#S5.SS1.SSS3" title="5.1.3 Metrics ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>), QD score AUC (Sec. <a href="#S6.SS2.SSS1" title="6.2.1 PGA-MAP-Elites and objective-measure space exploration ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.1</span></a>), archive coverage (Sec. <a href="#S5.SS1.SSS3" title="5.1.3 Metrics ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>), best performance (Sec. <a href="#S5.SS1.SSS3" title="5.1.3 Metrics ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>), mean elite robustness (Sec. <a href="#S6.SS2.SSS4" title="6.2.4 MAP-Elites and robustness ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.4</span></a>), and runtime in hours (Sec. <a href="#S6.SS2.SSS6" title="6.2.6 Computational effort ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.6</span></a>) for all algorithms in all environments. The tables show the value of each metric after 1 million evaluations, averaged over 5 trials. Due to its magnitude, QD score AUC is expressed as a multiple of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>12</mn></msup></mrow><annotation encoding="application/x-tex">10^{12}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>.</p></div><figure id="A4.T7" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>QD Score</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_border_tt ltx_th_row" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_t" style="text-align:left">CMA-MEGA (ES)</th><td class="ltx_td ltx_border_t" style="text-align:right">1,649,846.69</td><td class="ltx_td ltx_border_t" style="text-align:right">4,489,327.04</td><td class="ltx_td ltx_border_t" style="text-align:right">1,016,897.48</td><td class="ltx_td ltx_border_t" style="text-align:right">371,804.19</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">CMA-MEGA (TD3, ES)</th><td class="ltx_td" style="text-align:right">1,479,725.62</td><td class="ltx_td" style="text-align:right">4,612,926.99</td><td class="ltx_td" style="text-align:right">1,857,671.12</td><td class="ltx_td" style="text-align:right">1,437,319.62</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">PGA-MAP-Elites</th><td class="ltx_td" style="text-align:right">1,674,374.81</td><td class="ltx_td" style="text-align:right">4,758,921.89</td><td class="ltx_td" style="text-align:right">2,068,953.54</td><td class="ltx_td" style="text-align:right">1,480,443.84</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">ME-ES</th><td class="ltx_td" style="text-align:right">539,742.08</td><td class="ltx_td" style="text-align:right">2,296,974.58</td><td class="ltx_td" style="text-align:right">791,954.55</td><td class="ltx_td" style="text-align:right">105,320.97</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_bb" style="text-align:left">MAP-Elites</th><td class="ltx_td ltx_border_bb" style="text-align:right">1,418,306.56</td><td class="ltx_td ltx_border_bb" style="text-align:right">4,175,704.19</td><td class="ltx_td ltx_border_bb" style="text-align:right">1,835,703.73</td><td class="ltx_td ltx_border_bb" style="text-align:right">447,737.90</td></tr></tbody></table></figure><figure id="A4.T8" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>QD Score AUC (multiple of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>12</mn></msup></mrow><annotation encoding="application/x-tex">10^{12}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>)</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_border_tt ltx_th_row" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_t" style="text-align:left">CMA-MEGA (ES)</th><td class="ltx_td ltx_border_t" style="text-align:right">1.31</td><td class="ltx_td ltx_border_t" style="text-align:right">3.96</td><td class="ltx_td ltx_border_t" style="text-align:right">0.74</td><td class="ltx_td ltx_border_t" style="text-align:right">0.28</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">CMA-MEGA (TD3, ES)</th><td class="ltx_td" style="text-align:right">1.14</td><td class="ltx_td" style="text-align:right">3.97</td><td class="ltx_td" style="text-align:right">1.39</td><td class="ltx_td" style="text-align:right">1.01</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">PGA-MAP-Elites</th><td class="ltx_td" style="text-align:right">1.39</td><td class="ltx_td" style="text-align:right">4.39</td><td class="ltx_td" style="text-align:right">1.81</td><td class="ltx_td" style="text-align:right">1.04</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">ME-ES</th><td class="ltx_td" style="text-align:right">0.35</td><td class="ltx_td" style="text-align:right">1.57</td><td class="ltx_td" style="text-align:right">0.49</td><td class="ltx_td" style="text-align:right">0.07</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_bb" style="text-align:left">MAP-Elites</th><td class="ltx_td ltx_border_bb" style="text-align:right">1.18</td><td class="ltx_td ltx_border_bb" style="text-align:right">3.78</td><td class="ltx_td ltx_border_bb" style="text-align:right">1.34</td><td class="ltx_td ltx_border_bb" style="text-align:right">0.35</td></tr></tbody></table></figure><figure id="A4.T9" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Archive Coverage</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_border_tt ltx_th_row" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_t" style="text-align:left">CMA-MEGA (ES)</th><td class="ltx_td ltx_border_t" style="text-align:right">0.96</td><td class="ltx_td ltx_border_t" style="text-align:right">1.00</td><td class="ltx_td ltx_border_t" style="text-align:right">0.97</td><td class="ltx_td ltx_border_t" style="text-align:right">1.00</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">CMA-MEGA (TD3, ES)</th><td class="ltx_td" style="text-align:right">0.97</td><td class="ltx_td" style="text-align:right">1.00</td><td class="ltx_td" style="text-align:right">0.98</td><td class="ltx_td" style="text-align:right">1.00</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">PGA-MAP-Elites</th><td class="ltx_td" style="text-align:right">0.96</td><td class="ltx_td" style="text-align:right">1.00</td><td class="ltx_td" style="text-align:right">0.97</td><td class="ltx_td" style="text-align:right">0.99</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">ME-ES</th><td class="ltx_td" style="text-align:right">0.63</td><td class="ltx_td" style="text-align:right">0.95</td><td class="ltx_td" style="text-align:right">0.74</td><td class="ltx_td" style="text-align:right">0.86</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_bb" style="text-align:left">MAP-Elites</th><td class="ltx_td ltx_border_bb" style="text-align:right">0.98</td><td class="ltx_td ltx_border_bb" style="text-align:right">1.00</td><td class="ltx_td ltx_border_bb" style="text-align:right">0.98</td><td class="ltx_td ltx_border_bb" style="text-align:right">1.00</td></tr></tbody></table></figure><figure id="A4.T10" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Best Performance</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_border_tt ltx_th_row" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_t" style="text-align:left">CMA-MEGA (ES)</th><td class="ltx_td ltx_border_t" style="text-align:right">2,213.06</td><td class="ltx_td ltx_border_t" style="text-align:right">2,265.73</td><td class="ltx_td ltx_border_t" style="text-align:right">1,441.00</td><td class="ltx_td ltx_border_t" style="text-align:right">940.50</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">CMA-MEGA (TD3, ES)</th><td class="ltx_td" style="text-align:right">2,482.83</td><td class="ltx_td" style="text-align:right">2,486.10</td><td class="ltx_td" style="text-align:right">2,597.87</td><td class="ltx_td" style="text-align:right">2,302.31</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">PGA-MAP-Elites</th><td class="ltx_td" style="text-align:right">2,843.86</td><td class="ltx_td" style="text-align:right">2,746.98</td><td class="ltx_td" style="text-align:right">2,884.08</td><td class="ltx_td" style="text-align:right">2,619.17</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">ME-ES</th><td class="ltx_td" style="text-align:right">2,515.20</td><td class="ltx_td" style="text-align:right">1,911.33</td><td class="ltx_td" style="text-align:right">2,642.30</td><td class="ltx_td" style="text-align:right">1,025.74</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_bb" style="text-align:left">MAP-Elites</th><td class="ltx_td ltx_border_bb" style="text-align:right">1,506.97</td><td class="ltx_td ltx_border_bb" style="text-align:right">1,822.88</td><td class="ltx_td ltx_border_bb" style="text-align:right">2,602.94</td><td class="ltx_td ltx_border_bb" style="text-align:right">989.31</td></tr></tbody></table></figure><figure id="A4.T11" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Mean Elite Robustness</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_border_tt ltx_th_row" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_t" style="text-align:left">CMA-MEGA (ES)</th><td class="ltx_td ltx_border_t" style="text-align:right">-51.62</td><td class="ltx_td ltx_border_t" style="text-align:right">-105.81</td><td class="ltx_td ltx_border_t" style="text-align:right">-187.44</td><td class="ltx_td ltx_border_t" style="text-align:right">-86.45</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">CMA-MEGA (TD3, ES)</th><td class="ltx_td" style="text-align:right">-48.91</td><td class="ltx_td" style="text-align:right">-80.78</td><td class="ltx_td" style="text-align:right">-273.68</td><td class="ltx_td" style="text-align:right">-97.40</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">PGA-MAP-Elites</th><td class="ltx_td" style="text-align:right">-4.16</td><td class="ltx_td" style="text-align:right">-92.38</td><td class="ltx_td" style="text-align:right">-435.45</td><td class="ltx_td" style="text-align:right">-74.26</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">ME-ES</th><td class="ltx_td" style="text-align:right">77.76</td><td class="ltx_td" style="text-align:right">-645.40</td><td class="ltx_td" style="text-align:right">-631.32</td><td class="ltx_td" style="text-align:right">2.05</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_bb" style="text-align:left">MAP-Elites</th><td class="ltx_td ltx_border_bb" style="text-align:right">-109.42</td><td class="ltx_td ltx_border_bb" style="text-align:right">-338.78</td><td class="ltx_td ltx_border_bb" style="text-align:right">-509.21</td><td class="ltx_td ltx_border_bb" style="text-align:right">-186.14</td></tr></tbody></table></figure><figure id="A4.T12" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>Runtime (Hours)</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_border_tt ltx_th_row" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_t" style="text-align:left">CMA-MEGA (ES)</th><td class="ltx_td ltx_border_t" style="text-align:right">7.40</td><td class="ltx_td ltx_border_t" style="text-align:right">7.24</td><td class="ltx_td ltx_border_t" style="text-align:right">3.84</td><td class="ltx_td ltx_border_t" style="text-align:right">3.52</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">CMA-MEGA (TD3, ES)</th><td class="ltx_td" style="text-align:right">16.26</td><td class="ltx_td" style="text-align:right">22.79</td><td class="ltx_td" style="text-align:right">13.43</td><td class="ltx_td" style="text-align:right">13.01</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">PGA-MAP-Elites</th><td class="ltx_td" style="text-align:right">19.99</td><td class="ltx_td" style="text-align:right">19.75</td><td class="ltx_td" style="text-align:right">12.65</td><td class="ltx_td" style="text-align:right">12.86</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left" style="text-align:left">ME-ES</th><td class="ltx_td" style="text-align:right">8.92</td><td class="ltx_td" style="text-align:right">10.25</td><td class="ltx_td" style="text-align:right">4.04</td><td class="ltx_td" style="text-align:right">4.12</td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_row ltx_align_left ltx_border_bb" style="text-align:left">MAP-Elites</th><td class="ltx_td ltx_border_bb" style="text-align:right">7.43</td><td class="ltx_td ltx_border_bb" style="text-align:right">7.37</td><td class="ltx_td ltx_border_bb" style="text-align:right">4.59</td><td class="ltx_td ltx_border_bb" style="text-align:right">5.72</td></tr></tbody></table></figure></section><section id="A5" class="ltx_appendix"><h2 class="ltx_title ltx_title_appendix"><span class="ltx_tag ltx_tag_appendix">Appendix E </span>Full Statistical Analysis</h2><div id="A5.p1" class="ltx_para"><p class="ltx_p">To compare a metric such as QD score between two or more algorithms across all four QDGym environments, we performed a two-way ANOVA where environment and algorithm were the independent variables and the metric was the dependent variable. When there was a significant interaction effect (note that all of our analyses found significant interaction effects), we followed up this ANOVA with a simple main effects analysis in each environment. Finally, we ran pairwise comparisons (two-sided t-tests) to determine which algorithms had a significant difference on the metric. We applied Bonferroni corrections within each environment / simple main effect. For example, in Table <a href="#A5.T13" title="Table 13 ‣ E.3 Mean Elite Robustness Analysis (Sec. 6.2.4) ‣ Appendix E Full Statistical Analysis ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> we compared CMA-MEGA (ES) with three algorithms in each environment, so we applied a Bonferroni correction with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">n=3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">3</span></span></span></span>.</p></div><div id="A5.p2" class="ltx_para"><p class="ltx_p">This section lists the ANOVA and pairwise comparison results for each of our analyses. We have <span class="ltx_text ltx_font_bold">bolded</span> all significant <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal">p</span></span></span></span>-values, where significance is determined at the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding="application/x-tex">\alpha=0.05</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">5</span></span></span></span> threshold. For pairwise comparisons, some <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal">p</span></span></span></span>-values are marked as “1” because the Bonferroni correction caused the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal">p</span></span></span></span>-value to exceed 1. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal">p</span></span></span></span>-values less than 0.001 have been marked as “<span class="ltx_text ltx_font_bold">&lt; 0.001</span>”.</p></div><section id="A5.SS1" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">E.1 </span>QD Score Analysis (Sec. <a href="#S6.SS1" title="6.1 Analysis ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>)</h3><div id="A5.SS1.p1" class="ltx_para"><p class="ltx_p">To test the hypotheses we defined in Sec. <a href="#S5.SS2" title="5.2 Experimental Design ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, we performed a two-way ANOVA for QD scores. Since the ANOVA requires scores in all environments to have the same scale, we normalized the QD score in all environments by dividing by the maximum QD score, defined in Sec. <a href="#S6.SS1" title="6.1 Analysis ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a> as <span class="ltx_text ltx_font_italic">grid cells</span> * (<span class="ltx_text ltx_font_italic">max objective</span> - <span class="ltx_text ltx_font_italic">min objective</span>). The results of the ANOVA were as follows:</p></div><div id="A5.SS1.p2" class="ltx_para"><ul id="A5.I1" class="list-disc ltx_itemize"><li id="A5.I1.i1" class="ltx_item"><div id="A5.I1.i1.p1" class="ltx_para"><p class="ltx_p">Interaction effect: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>12</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>16.82</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(12,80)=16.82,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">1</span><span class="mord">6</span><span class="mord">.</span><span class="mord">8</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I1.i2" class="ltx_item"><div id="A5.I1.i2.p1" class="ltx_para"><p class="ltx_p">Simple main effects:</p><ul id="A5.I1.I1" class="list-disc ltx_itemize"><li id="A5.I1.i2.i1" class="ltx_item"><div id="A5.I1.i2.i1.p1" class="ltx_para"><p class="ltx_p">QD Ant: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>23.87</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=23.87,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">2</span><span class="mord">3</span><span class="mord">.</span><span class="mord">8</span><span class="mord">7</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I1.i2.i2" class="ltx_item"><div id="A5.I1.i2.i2.p1" class="ltx_para"><p class="ltx_p">QD Half-Cheetah: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>44.15</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=44.15,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">4</span><span class="mord">4</span><span class="mord">.</span><span class="mord">1</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I1.i2.i3" class="ltx_item"><div id="A5.I1.i2.i3.p1" class="ltx_para"><p class="ltx_p">QD Hopper: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>57.35</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=57.35,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">5</span><span class="mord">7</span><span class="mord">.</span><span class="mord">3</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I1.i2.i4" class="ltx_item"><div id="A5.I1.i2.i4.p1" class="ltx_para"><p class="ltx_p">QD Walker: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>90.84</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=90.84,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">9</span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li></ul></div></li></ul></div><div id="A5.SS1.p3" class="ltx_para"><p class="ltx_p">Since the ANOVA showed a significant interaction effect and significant simple main effects, we performed pairwise comparisons for each hypothesis (Tables <a href="#A5.T13" title="Table 13 ‣ E.3 Mean Elite Robustness Analysis (Sec. 6.2.4) ‣ Appendix E Full Statistical Analysis ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>-<a href="#A5.T15" title="Table 15 ‣ E.3 Mean Elite Robustness Analysis (Sec. 6.2.4) ‣ Appendix E Full Statistical Analysis ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>).</p></div></section><section id="A5.SS2" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">E.2 </span>QD Score AUC Analysis (Sec. <a href="#S6.SS2.SSS1" title="6.2.1 PGA-MAP-Elites and objective-measure space exploration ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.1</span></a>)</h3><div id="A5.SS2.p1" class="ltx_para"><p class="ltx_p">In this followup analysis, we hypothesized that PGA-MAP-Elites would have greater QD score AUC than CMA-MEGA (ES) and CMA-MEGA (TD3, ES). Thus, we performed a two-way ANOVA which compared QD score AUC for PGA-MAP-Elites, CMA-MEGA (ES), and CMA-MEGA (TD3, ES). As we did for QD score, we normalized QD score AUC by the maximum QD score. The ANOVA results were as follows:</p></div><div id="A5.SS2.p2" class="ltx_para"><ul id="A5.I2" class="list-disc ltx_itemize"><li id="A5.I2.i1" class="ltx_item"><div id="A5.I2.i1.p1" class="ltx_para"><p class="ltx_p">Interaction effect: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>12</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>17.55</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(12,80)=17.55,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">1</span><span class="mord">7</span><span class="mord">.</span><span class="mord">5</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I2.i2" class="ltx_item"><div id="A5.I2.i2.p1" class="ltx_para"><p class="ltx_p">Simple main effects:</p><ul id="A5.I2.I1" class="list-disc ltx_itemize"><li id="A5.I2.i2.i1" class="ltx_item"><div id="A5.I2.i2.i1.p1" class="ltx_para"><p class="ltx_p">QD Ant: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>31.77</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=31.77,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">3</span><span class="mord">1</span><span class="mord">.</span><span class="mord">7</span><span class="mord">7</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I2.i2.i2" class="ltx_item"><div id="A5.I2.i2.i2.p1" class="ltx_para"><p class="ltx_p">QD Half-Cheetah: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>89.38</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=89.38,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">8</span><span class="mord">9</span><span class="mord">.</span><span class="mord">3</span><span class="mord">8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I2.i2.i3" class="ltx_item"><div id="A5.I2.i2.i3.p1" class="ltx_para"><p class="ltx_p">QD Hopper: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>82.34</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=82.34,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">8</span><span class="mord">2</span><span class="mord">.</span><span class="mord">3</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I2.i2.i4" class="ltx_item"><div id="A5.I2.i2.i4.p1" class="ltx_para"><p class="ltx_p">QD Walker: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>71.64</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=71.64,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">7</span><span class="mord">1</span><span class="mord">.</span><span class="mord">6</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li></ul></div></li></ul></div><div id="A5.SS2.p3" class="ltx_para"><p class="ltx_p">As the interaction and simple main effects were significant, we performed pairwise comparisons (Table <a href="#A5.T16" title="Table 16 ‣ E.3 Mean Elite Robustness Analysis (Sec. 6.2.4) ‣ Appendix E Full Statistical Analysis ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>).</p></div></section><section id="A5.SS3" class="ltx_subsection"><h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">E.3 </span>Mean Elite Robustness Analysis (Sec. <a href="#S6.SS2.SSS4" title="6.2.4 MAP-Elites and robustness ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.4</span></a>)</h3><div id="A5.SS3.p1" class="ltx_para"><p class="ltx_p">In this followup analysis, we hypothesized that MAP-Elites would have lower mean elite robustness than CMA-MEGA (ES) and CMA-MEGA (TD3, ES). Thus, we performed a two-way ANOVA which compared mean elite robustness for MAP-Elites, CMA-MEGA (ES), and CMA-MEGA (TD3, ES). We normalized by the score range, i.e. <span class="ltx_text ltx_font_italic">max objective</span> - <span class="ltx_text ltx_font_italic">min objective</span>. The ANOVA results were as follows:</p></div><div id="A5.SS3.p2" class="ltx_para"><ul id="A5.I3" class="list-disc ltx_itemize"><li id="A5.I3.i1" class="ltx_item"><div id="A5.I3.i1.p1" class="ltx_para"><p class="ltx_p">Interaction effect: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>12</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>8.75</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(12,80)=8.75,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">8</span><span class="mord">.</span><span class="mord">7</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I3.i2" class="ltx_item"><div id="A5.I3.i2.p1" class="ltx_para"><p class="ltx_p">Simple main effects:</p><ul id="A5.I3.I1" class="list-disc ltx_itemize"><li id="A5.I3.i2.i1" class="ltx_item"><div id="A5.I3.i2.i1.p1" class="ltx_para"><p class="ltx_p">QD Ant: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>3.17</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>=</mo><mn mathvariant="bold">0.018</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=3.17,\mathbf{p=0.018}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">3</span><span class="mord">.</span><span class="mord">1</span><span class="mord">7</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span><span class="mord mathbf">8</span></span></span></span></span></p></div></li><li id="A5.I3.i2.i2" class="ltx_item"><div id="A5.I3.i2.i2.p1" class="ltx_para"><p class="ltx_p">QD Half-Cheetah: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>9.60</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=9.60,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">9</span><span class="mord">.</span><span class="mord">6</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I3.i2.i3" class="ltx_item"><div id="A5.I3.i2.i3.p1" class="ltx_para"><p class="ltx_p">QD Hopper: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>21.07</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>&lt;</mo><mn mathvariant="bold">0.001</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=21.07,\mathbf{p&lt;0.001}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">2</span><span class="mord">1</span><span class="mord">.</span><span class="mord">0</span><span class="mord">7</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">1</span></span></span></span></span></p></div></li><li id="A5.I3.i2.i4" class="ltx_item"><div id="A5.I3.i2.i4.p1" class="ltx_para"><p class="ltx_p">QD Walker: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>80</mn><mo stretchy="false">)</mo><mo>=</mo><mn>3.70</mn><mo separator="true">,</mo><mrow><mi mathvariant="bold">p</mi><mo>=</mo><mn mathvariant="bold">0.008</mn></mrow></mrow><annotation encoding="application/x-tex">F(4,80)=3.70,\mathbf{p=0.008}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">8</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">3</span><span class="mord">.</span><span class="mord">7</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathbf">0</span><span class="mord mathbf">.</span><span class="mord mathbf">0</span><span class="mord mathbf">0</span><span class="mord mathbf">8</span></span></span></span></span></p></div></li></ul></div></li></ul></div><div id="A5.SS3.p3" class="ltx_para"><p class="ltx_p">As the interaction and simple main effects were significant, we performed pairwise comparisons (Table <a href="#A5.T17" title="Table 17 ‣ E.3 Mean Elite Robustness Analysis (Sec. 6.2.4) ‣ Appendix E Full Statistical Analysis ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>).</p></div><figure id="A5.T13" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 13: </span>H1 - Comparing QD score between CMA-MEGA (ES) and baselines</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column" style="text-align:left">Algorithm 1</th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left">Algorithm 2</th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left"></th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td ltx_border_bb ltx_border_t" rowspan="3" style="text-align:left"><span class="ltx_text ltx_align_left">CMA-MEGA (ES)</span></td><td class="ltx_td ltx_border_t" style="text-align:left">PGA-MAP-Elites</td><td class="ltx_td ltx_border_t" style="text-align:right">1</td><td class="ltx_td ltx_border_t" style="text-align:right">0.733</td><td class="ltx_td ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">0.003</span></td><td class="ltx_td ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left">ME-ES</td><td class="ltx_td" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td><td class="ltx_td" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td><td class="ltx_td" style="text-align:right">0.841</td><td class="ltx_td" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb" style="text-align:left">MAP-Elites</td><td class="ltx_td ltx_border_bb" style="text-align:right">0.254</td><td class="ltx_td ltx_border_bb" style="text-align:right">0.215</td><td class="ltx_td ltx_border_bb" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">0.007</span></td><td class="ltx_td ltx_border_bb" style="text-align:right">0.108</td></tr></tbody></table></figure><figure id="A5.T14" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 14: </span>H2 - Comparing QD score between CMA-MEGA (TD3, ES) and baselines</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column" style="text-align:left">Algorithm 1</th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left">Algorithm 2</th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column" style="text-align:left"></th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td ltx_border_bb ltx_border_t" rowspan="3" style="text-align:left"><span class="ltx_text ltx_align_left">CMA-MEGA (TD3, ES)</span></td><td class="ltx_td ltx_border_t" style="text-align:left">PGA-MAP-Elites</td><td class="ltx_td ltx_border_t" style="text-align:right">0.093</td><td class="ltx_td ltx_border_t" style="text-align:right">1</td><td class="ltx_td ltx_border_t" style="text-align:right">0.726</td><td class="ltx_td ltx_border_t" style="text-align:right">1</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left">ME-ES</td><td class="ltx_td" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td><td class="ltx_td" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td><td class="ltx_td" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td><td class="ltx_td" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb" style="text-align:left">MAP-Elites</td><td class="ltx_td ltx_border_bb" style="text-align:right">1</td><td class="ltx_td ltx_border_bb" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">0.010</span></td><td class="ltx_td ltx_border_bb" style="text-align:right">1</td><td class="ltx_td ltx_border_bb" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td></tr></tbody></table></figure><figure id="A5.T15" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 15: </span>H3 - Comparing QD score between CMA-MEGA (ES) and CMA-MEGA (TD3, ES)</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular"><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td ltx_border_tt" style="text-align:left"></td><td class="ltx_td ltx_border_tt" style="text-align:left"></td><td class="ltx_td ltx_border_tt" style="text-align:left">QD Ant</td><td class="ltx_td ltx_border_tt" style="text-align:left">QD Half-Cheetah</td><td class="ltx_td ltx_border_tt" style="text-align:left">QD Hopper</td><td class="ltx_td ltx_border_tt" style="text-align:left">QD Walker</td></tr><tr class="ltx_tr"><td class="ltx_td" style="text-align:left">Algorithm 1</td><td class="ltx_td" style="text-align:left">Algorithm 2</td><td class="ltx_td" style="text-align:left"></td><td class="ltx_td" style="text-align:left"></td><td class="ltx_td" style="text-align:left"></td><td class="ltx_td" style="text-align:left"></td></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb ltx_border_t" style="text-align:left">CMA-MEGA (ES)</td><td class="ltx_td ltx_border_bb ltx_border_t" style="text-align:left">CMA-MEGA (TD3, ES)</td><td class="ltx_td ltx_border_bb ltx_border_t" style="text-align:right">0.250</td><td class="ltx_td ltx_border_bb ltx_border_t" style="text-align:right">0.511</td><td class="ltx_td ltx_border_bb ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">0.006</span></td><td class="ltx_td ltx_border_bb ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td></tr></tbody></table></figure><figure id="A5.T16" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 16: </span>Comparing QD score AUC between PGA-ME and CMA-MEGA variants</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td" style="text-align:left">Algorithm 1</td><td class="ltx_td" style="text-align:left">Algorithm 2</td><td class="ltx_td" style="text-align:left"></td><td class="ltx_td" style="text-align:left"></td><td class="ltx_td" style="text-align:left"></td><td class="ltx_td" style="text-align:left"></td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_t ltx_border_bb" rowspan="2" style="text-align:left"><span class="ltx_text ltx_align_left">PGA-MAP-Elites</span></th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:left">CMA-MEGA (ES)</th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:right">0.734</th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:right">0.255</th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></th></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb" style="text-align:left">CMA-MEGA (TD3, ES)</td><td class="ltx_td ltx_border_bb" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">0.020</span></td><td class="ltx_td ltx_border_bb" style="text-align:right">0.111</td><td class="ltx_td ltx_border_bb" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">0.003</span></td><td class="ltx_td ltx_border_bb" style="text-align:right">1</td></tr></tbody></table></figure><figure id="A5.T17" class="ltx_table"><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 17: </span>Comparing mean elite robustness between MAP-Elites and CMA-MEGA variants</figcaption><table class="ltx_align_middle ltx_centering ltx_tabular ltx_guessed_headers"><thead class="ltx_thead"><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Ant</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Half-Cheetah</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Hopper</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="text-align:left">QD Walker</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr"><td class="ltx_td" style="text-align:left">Algorithm 1</td><td class="ltx_td" style="text-align:left">Algorithm 2</td><td class="ltx_td" style="text-align:left"></td><td class="ltx_td" style="text-align:left"></td><td class="ltx_td" style="text-align:left"></td><td class="ltx_td" style="text-align:left"></td></tr><tr class="ltx_tr"><th class="ltx_td ltx_th ltx_th_column ltx_border_t ltx_border_bb" rowspan="2" style="text-align:left"><span class="ltx_text ltx_align_left">MAP-Elites</span></th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:left">CMA-MEGA (ES)</th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">0.030</span></th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">0.003</span></th></tr><tr class="ltx_tr"><td class="ltx_td ltx_border_bb" style="text-align:left">CMA-MEGA (TD3, ES)</td><td class="ltx_td ltx_border_bb" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td><td class="ltx_td ltx_border_bb" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td><td class="ltx_td ltx_border_bb" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">0.013</span></td><td class="ltx_td ltx_border_bb" style="text-align:right"><span class="ltx_text ltx_font_bold ltx_align_right ltx_wrap">&lt; 0.001</span></td></tr></tbody></table></figure></section></section><section id="A6" class="ltx_appendix"><h2 class="ltx_title ltx_title_appendix"><span class="ltx_tag ltx_tag_appendix">Appendix F </span>Archive Visualizations</h2><div id="A6.p1" class="ltx_para"><p class="ltx_p">We visualize “median” archives in Fig. <a href="#A6.F4" title="Figure 4 ‣ Appendix F Archive Visualizations ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and Fig. <a href="#A6.F5" title="Figure 5 ‣ Appendix F Archive Visualizations ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. To determine these median archives, we selected the trial which achieved the median QD score out of the 5 trials of each algorithm in each environment. Fig. <a href="#A6.F4" title="Figure 4 ‣ Appendix F Archive Visualizations ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> visualizes heatmaps of median archives in QD Half-Cheetah and QD Walker, while Fig. <a href="#A6.F5" title="Figure 5 ‣ Appendix F Archive Visualizations ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the distribution (histogram) of objective values for median archives in all environments.</p></div><figure id="A6.F4" class="mx-auto max-w-screen-md ltx_figure mount_heatmaps"><video src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" class="w-full b-lazy mb-2 rounded-lg shadow-lg" type="video/mp4" autoplay="" controls="" muted="" playsinline="" loop="" preload="auto" data-src="static/heatmap_video.mp4"></video><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Archive heatmaps from the median trial (in terms of QD score) of each algorithm in QD Half-Cheetah and QD Walker. The colorbar for each environment ranges from the minimum to maximum objective stated in Table <a href="#S5.T1" title="Table 1 ‣ 5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The archive in both environments is a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32\times 32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">3</span><span class="mord">2</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">3</span><span class="mord">2</span></span></span></span> grid. Currently, we are unable to plot heatmaps for QD Ant and QD Hopper because their archives are not 2D.</figcaption></figure><figure id="A6.F5" class="mx-auto max-w-screen-md ltx_figure mount_histograms"><video src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" class="w-full b-lazy mb-2 rounded-lg shadow-lg" type="video/mp4" autoplay="" controls="" muted="" playsinline="" loop="" preload="auto" data-src="static/histogram_video.mp4"></video><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Distribution (histogram) of objective values in archives from the median trial (in terms of QD score) of each algorithm in each environment. In each plot, the x-axis is bounded on the left by the minimum objective and on the right by the maximum objective plus 400, as some solutions exceed the maximum objective in Table <a href="#S5.T1" title="Table 1 ‣ 5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Note that in some plots, the number of items overflows the y-axis bounds (e.g. ME-ES in QD Walker).</figcaption></figure></section><section id="bib" class="ltx_bibliography"><h2 class="ltx_title ltx_title_bibliography">References</h2><ul id="bib.L1" class="ltx_biblist"><li id="bib.bib58" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Akimoto, Y. Nagata, I. Ono, and S. Kobayashi</span><span class="ltx_text ltx_bib_year"> (2010)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bidirectional relation between cma evolution strategies and natural evolution strategies</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Parallel Problem Solving from Nature, PPSN XI</span>, <span class="ltx_text ltx_bib_editor">R. Schaefer, C. Cotta, J. Kołodziej, and G. Rudolph (Eds.)</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Berlin, Heidelberg</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;154–163</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external isbn">ISBN 978-3-642-15844-5</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.</span></li><li id="bib.bib61" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Amari</span><span class="ltx_text ltx_bib_year"> (1998-02)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Natural Gradient Works Efficiently in Learning</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Neural Computation</span> <span class="ltx_text ltx_bib_volume">10</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages">pp.&nbsp;251–276</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external issn">ISSN 0899-7667</span>, <a href="http://dx.doi.org/10.1162/089976698300017746" title="" class="ltx_ref ltx_bib_external doi">Document</a>, <a href="https://doi.org/10.1162/089976698300017746" title="" class="ltx_ref ltx_bib_external">Link</a>, <span class="ltx_text ltx_bib_external">https://direct.mit.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.SSS1.p1" title="3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.1</span></a>.</span></li><li id="bib.bib30" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba</span><span class="ltx_text ltx_bib_year"> (2017)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Hindsight experience replay</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>, <span class="ltx_text ltx_bib_editor">I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.)</span>, </span><span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">30</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.neurips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p1" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib63" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Beyer and H. Schwefel</span><span class="ltx_text ltx_bib_year"> (2002-03-01)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evolution strategies – a comprehensive introduction</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Natural Computing</span> <span class="ltx_text ltx_bib_volume">1</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages">pp.&nbsp;3–52</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external issn">ISSN 1572-9796</span>, <a href="http://dx.doi.org/10.1023/A%3A1015059928466" title="" class="ltx_ref ltx_bib_external doi">Document</a>, <a href="https://doi.org/10.1023/A:1015059928466" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.SSS1.p1" title="3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.1</span></a>.</span></li><li id="bib.bib11" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba</span><span class="ltx_text ltx_bib_year"> (2016)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">OpenAI gym</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1606.01540</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1606.01540" title="" class="ltx_ref ltx_bib_external">Link</a>, <span class="ltx_text ltx_bib_external">1606.01540</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.SSS1.p1" title="5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1.1</span></a>.</span></li><li id="bib.bib43" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Cideron, T. Pierrot, N. Perrin, K. Beguir, and O. Sigaud</span><span class="ltx_text ltx_bib_year"> (2020)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">QD-RL: efficient mixing of quality and diversity in reinforcement learning</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/2006.08505</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://arxiv.org/abs/2006.08505" title="" class="ltx_ref ltx_bib_external">Link</a>, <span class="ltx_text ltx_bib_external">2006.08505</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS3.p1" title="3.2.3 Beyond MAP-Elites ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.3</span></a>.</span></li><li id="bib.bib54" class="ltx_bibitem ltx_bib_misc"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Clark and D. Amodei</span><span class="ltx_text ltx_bib_year"> (2016)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Faulty reward functions in the wild</span>. </span><span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a href="https://openai.com/blog/faulty-reward-functions/" title="" class="ltx_ref ltx_font_typewriter ltx_url">https://openai.com/blog/faulty-reward-functions/</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.</span></li><li id="bib.bib4" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Colas, V. Madhavan, J. Huizinga, and J. Clune</span><span class="ltx_text ltx_bib_year"> (2020)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scaling map-elites to deep neuroevolution</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 2020 Genetic and Evolutionary Computation Conference</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">GECCO ’20</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;67–75</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external isbn">ISBN 9781450371285</span>, <a href="https://doi.org/10.1145/3377930.3390217" title="" class="ltx_ref ltx_bib_external">Link</a>, <a href="http://dx.doi.org/10.1145/3377930.3390217" title="" class="ltx_ref ltx_bib_external doi">Document</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S1.p4" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S3.SS2.SSS1.p3" title="3.2.1 MAP-Elites extensions for QD-RL ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.1</span></a>, <a href="#S5.p1" title="5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.</span></li><li id="bib.bib32" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Colas, O. Sigaud, and P. Oudeyer</span><span class="ltx_text ltx_bib_year"> (2018-10–15 Jul)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">GEP-PG: decoupling exploration and exploitation in deep reinforcement learning algorithms</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 35th International Conference on Machine Learning</span>, <span class="ltx_text ltx_bib_editor">J. Dy and A. Krause (Eds.)</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">80</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;1039–1048</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.mlr.press/v80/colas18a.html" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib31" class="ltx_bibitem ltx_bib_incollection"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. Conti, V. Madhavan, F. Petroski Such, J. Lehman, K. Stanley, and J. Clune</span><span class="ltx_text ltx_bib_year"> (2018)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems 31</span>, <span class="ltx_text ltx_bib_editor">S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.)</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages">pp.&nbsp;5027–5038</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://papers.nips.cc/paper/7750-improving-exploration-in-evolution-strategies-for-deep-reinforcement-learning-via-a-population-of-novelty-seeking-agents.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS3.p1" title="3.2.3 Beyond MAP-Elites ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.3</span></a>.</span></li><li id="bib.bib37" class="ltx_bibitem ltx_bib_misc"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. Coumans and Y. Bai</span><span class="ltx_text ltx_bib_year"> (2016–2020)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">PyBullet, a python module for physics simulation for games, robotics and machine learning</span>. </span><span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a href="http://pybullet.org" title="" class="ltx_ref ltx_font_typewriter ltx_url">http://pybullet.org</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.SSS1.p1" title="5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1.1</span></a>.</span></li><li id="bib.bib7" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Cully, J. Clune, D. Tarapore, and J. Mouret</span><span class="ltx_text ltx_bib_year"> (2015-05)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Robots that can adapt like animals</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature</span> <span class="ltx_text ltx_bib_volume">521</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;503–507</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1038/nature14422" title="" class="ltx_ref ltx_bib_external doi">Document</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S1.p4" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S2.SS1.p1" title="2.1 Quality Diversity (QD) ‣ 2 Problem Statement ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>, <a href="#S3.SS2.SSS1.p1" title="3.2.1 MAP-Elites extensions for QD-RL ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.1</span></a>.</span></li><li id="bib.bib49" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. de Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein</span><span class="ltx_text ltx_bib_year"> (2005-02-01)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A tutorial on the cross-entropy method</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Annals of Operations Research</span> <span class="ltx_text ltx_bib_volume">134</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages">pp.&nbsp;19–67</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external issn">ISSN 1572-9338</span>, <a href="http://dx.doi.org/10.1007/s10479-005-5724-z" title="" class="ltx_ref ltx_bib_external doi">Document</a>, <a href="https://doi.org/10.1007/s10479-005-5724-z" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib38" class="ltx_bibitem ltx_bib_misc"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Ellenberger</span><span class="ltx_text ltx_bib_year"> (2018–2019)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">PyBullet gymperium</span>. </span><span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a href="https://github.com/benelot/pybullet-gym" title="" class="ltx_ref ltx_font_typewriter ltx_url">https://github.com/benelot/pybullet-gym</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p6" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S5.SS1.SSS1.p1" title="5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1.1</span></a>.</span></li><li id="bib.bib27" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine</span><span class="ltx_text ltx_bib_year"> (2019)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Diversity is all you need: learning skills without a reward function</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Learning Representations</span>, </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://openreview.net/forum?id=SJx63jRqFm" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p1" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib3" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. C. Fontaine and S. Nikolaidis</span><span class="ltx_text ltx_bib_year"> (2021)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Differentiable quality diversity</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advances in Neural Information Processing Systems</span> <span class="ltx_text ltx_bib_volume">34</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.neurips.cc/paper/2021/file/532923f11ac97d3e7cb0130315b067dc-Paper.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S1.p5" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S1.p7" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S2.SS1.p1" title="2.1 Quality Diversity (QD) ‣ 2 Problem Statement ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>, <a href="#S3.SS2.SSS2.p1" title="3.2.2 Covariance Matrix Adaptation MAP-Elites via a Gradient Arborescence (CMA-MEGA) ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.2</span></a>, <a href="#S5.SS2.p1" title="5.2 Experimental Design ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>, <a href="#S6.SS2.SSS1.p1" title="6.2.1 PGA-MAP-Elites and objective-measure space exploration ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.2.1</span></a>, <a href="#S7.p2" title="7 Conclusion ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§7</span></a>.</span></li><li id="bib.bib5" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. C. Fontaine, J. Togelius, S. Nikolaidis, and A. K. Hoover</span><span class="ltx_text ltx_bib_year"> (2020)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Covariance matrix adaptation for the rapid illumination of behavior space</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 2020 Genetic and Evolutionary Computation Conference</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">GECCO ’20</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;94–102</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external isbn">ISBN 9781450371285</span>, <a href="https://doi.org/10.1145/3377930.3390232" title="" class="ltx_ref ltx_bib_external">Link</a>, <a href="http://dx.doi.org/10.1145/3377930.3390232" title="" class="ltx_ref ltx_bib_external doi">Document</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS2.p1" title="3.2.2 Covariance Matrix Adaptation MAP-Elites via a Gradient Arborescence (CMA-MEGA) ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.2</span></a>, <a href="#S6.SS2.SSS1.p2" title="6.2.1 PGA-MAP-Elites and objective-measure space exploration ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.2.1</span></a>.</span></li><li id="bib.bib17" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Fujimoto, H. van Hoof, and D. Meger</span><span class="ltx_text ltx_bib_year"> (2018-10–15 Jul)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Addressing function approximation error in actor-critic methods</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 35th International Conference on Machine Learning</span>, <span class="ltx_text ltx_bib_editor">J. Dy and A. Krause (Eds.)</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">80</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;1587–1596</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://proceedings.mlr.press/v80/fujimoto18a.html" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S1.p6" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S3.SS1.SSS2.p1" title="3.1.2 Actor-critic methods ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.2</span></a>, <a href="#S4.SS1.SSS1.p3" title="4.1.1 Approximating objective gradients with ES and actor-critic methods ‣ 4.1 Approximating Objective and Measure Gradients ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1.1</span></a>, <a href="#S4.SS2.p4" title="4.2 CMA-MEGA Variants ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>, <a href="#algorithm3" title="Algorithm 3 ‣ Appendix A Helper Methods for CMA-MEGA Variants ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</span></li><li id="bib.bib39" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[19]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Glorot and Y. Bengio</span><span class="ltx_text ltx_bib_year"> (2010-13–15 May)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Understanding the difficulty of training deep feedforward neural networks</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</span>, <span class="ltx_text ltx_bib_editor">Y. W. Teh and M. Titterington (Eds.)</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">9</span>, <span class="ltx_text ltx_bib_place">Chia Laguna Resort, Sardinia, Italy</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;249–256</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.mlr.press/v9/glorot10a.html" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.SSS2.p1" title="5.1.2 Hyperparameters ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1.2</span></a>.</span></li><li id="bib.bib24" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[20]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Ha</span><span class="ltx_text ltx_bib_year"> (2017)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A visual guide to evolution strategies</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">blog.otoro.net</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://blog.otoro.net/2017/10/29/visual-evolution-strategies/" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.SSS1.p2" title="3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.1</span></a>.</span></li><li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine</span><span class="ltx_text ltx_bib_year"> (2018-10–15 Jul)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 35th International Conference on Machine Learning</span>, <span class="ltx_text ltx_bib_editor">J. Dy and A. Krause (Eds.)</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">80</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;1861–1870</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.mlr.press/v80/haarnoja18b.html" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.SSS1.p3" title="4.1.1 Approximating objective gradients with ES and actor-critic methods ‣ 4.1 Approximating Objective and Measure Gradients ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1.1</span></a>.</span></li><li id="bib.bib6" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[22]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Hansen</span><span class="ltx_text ltx_bib_year"> (2016)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The CMA evolution strategy: A tutorial</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1604.00772</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1604.00772" title="" class="ltx_ref ltx_bib_external">Link</a>, <span class="ltx_text ltx_bib_external">1604.00772</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p4" title="3.2.1 MAP-Elites extensions for QD-RL ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.1</span></a>, <a href="#S3.SS2.SSS2.p2" title="3.2.2 Covariance Matrix Adaptation MAP-Elites via a Gradient Arborescence (CMA-MEGA) ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.2</span></a>.</span></li><li id="bib.bib53" class="ltx_bibitem ltx_bib_misc"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[23]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Irpan</span><span class="ltx_text ltx_bib_year"> (2018)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep reinforcement learning doesn’t work yet</span>. </span><span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a href="https://www.alexirpan.com/2018/02/14/rl-hard.html" title="" class="ltx_ref ltx_font_typewriter ltx_url">https://www.alexirpan.com/2018/02/14/rl-hard.html</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.</span></li><li id="bib.bib51" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[24]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Khadka, S. Majumdar, T. Nassar, Z. Dwiel, E. Tumer, S. Miret, Y. Liu, and K. Tumer</span><span class="ltx_text ltx_bib_year"> (2019-09–15 Jun)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Collaborative evolutionary reinforcement learning</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 36th International Conference on Machine Learning</span>, <span class="ltx_text ltx_bib_editor">K. Chaudhuri and R. Salakhutdinov (Eds.)</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">97</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;3341–3350</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.mlr.press/v97/khadka19a.html" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib34" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Khadka and K. Tumer</span><span class="ltx_text ltx_bib_year"> (2018)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evolution-guided policy gradient in reinforcement learning</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>, <span class="ltx_text ltx_bib_editor">S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.)</span>, </span><span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">31</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.neurips.cc/paper/2018/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib12" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[26]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. P. Kingma and J. Ba</span><span class="ltx_text ltx_bib_year"> (2015)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Adam: A method for stochastic optimization</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</span>, <span class="ltx_text ltx_bib_editor">Y. Bengio and Y. LeCun (Eds.)</span>, </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1412.6980" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.SSS1.p2" title="3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.1</span></a>.</span></li><li id="bib.bib28" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[27]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Kumar, A. Kumar, S. Levine, and C. Finn</span><span class="ltx_text ltx_bib_year"> (2020)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">One solution is not all you need: few-shot extrapolation via structured maxent rl</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advances in Neural Information Processing Systems</span> <span class="ltx_text ltx_bib_volume">33</span>. </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p1" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib22" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[28]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Lehman, J. Chen, J. Clune, and K. O. Stanley</span><span class="ltx_text ltx_bib_year"> (2018)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ES is more than just a traditional finite-difference approximator</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Genetic and Evolutionary Computation Conference</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">GECCO ’18</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;450–457</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external isbn">ISBN 9781450356183</span>, <a href="https://doi.org/10.1145/3205455.3205474" title="" class="ltx_ref ltx_bib_external">Link</a>, <a href="http://dx.doi.org/10.1145/3205455.3205474" title="" class="ltx_ref ltx_bib_external doi">Document</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.SSS1.p1" title="4.1.1 Approximating objective gradients with ES and actor-critic methods ‣ 4.1 Approximating Objective and Measure Gradients ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1.1</span></a>.</span></li><li id="bib.bib41" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[29]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Lehman and K. O. Stanley</span><span class="ltx_text ltx_bib_year"> (2011-06)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Abandoning Objectives: Evolution Through the Search for Novelty Alone</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Evolutionary Computation</span> <span class="ltx_text ltx_bib_volume">19</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages">pp.&nbsp;189–223</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external issn">ISSN 1063-6560</span>, <a href="http://dx.doi.org/10.1162/EVCO%5Fa%5F00025" title="" class="ltx_ref ltx_bib_external doi">Document</a>, <a href="https://doi.org/10.1162/EVCO%5C_a%5C_00025" title="" class="ltx_ref ltx_bib_external">Link</a>, <span class="ltx_text ltx_bib_external">https://direct.mit.edu/evco/article-pdf/19/2/189/1494066/evco_a_00025.pdf</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS3.p1" title="3.2.3 Beyond MAP-Elites ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.3</span></a>.</span></li><li id="bib.bib42" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[30]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Lehman and K. O. Stanley</span><span class="ltx_text ltx_bib_year"> (2011)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evolving a diversity of virtual creatures through novelty search and local competition</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">GECCO ’11</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;211–218</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external isbn">ISBN 9781450305570</span>, <a href="https://doi.org/10.1145/2001576.2001606" title="" class="ltx_ref ltx_bib_external">Link</a>, <a href="http://dx.doi.org/10.1145/2001576.2001606" title="" class="ltx_ref ltx_bib_external doi">Document</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS3.p1" title="3.2.3 Beyond MAP-Elites ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.3</span></a>.</span></li><li id="bib.bib29" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[31]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Li, J. Song, and S. Ermon</span><span class="ltx_text ltx_bib_year"> (2017)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">InfoGAIL: interpretable imitation learning from visual demonstrations</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>, <span class="ltx_text ltx_bib_editor">I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.)</span>, </span><span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">30</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.neurips.cc/paper/2017/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p1" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[32]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra</span><span class="ltx_text ltx_bib_year"> (2016)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Continuous control with deep reinforcement learning</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings</span>, <span class="ltx_text ltx_bib_editor">Y. Bengio and Y. LeCun (Eds.)</span>, </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1509.02971" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.</span></li><li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[33]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Mania, A. Guy, and B. Recht</span><span class="ltx_text ltx_bib_year"> (2018)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Simple random search of static linear policies is competitive for reinforcement learning</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 32nd International Conference on Neural Information Processing Systems</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">NIPS’18</span>, <span class="ltx_text ltx_bib_place">Red Hook, NY, USA</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;1805–1814</span>. </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S3.SS1.SSS1.p3" title="3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.1</span></a>.</span></li><li id="bib.bib8" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Mouret and J. Clune</span><span class="ltx_text ltx_bib_year"> (2015)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Illuminating search spaces by mapping elites</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1504.04909</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1504.04909" title="" class="ltx_ref ltx_bib_external">Link</a>, <span class="ltx_text ltx_bib_external">1504.04909</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S2.SS1.p1" title="2.1 Quality Diversity (QD) ‣ 2 Problem Statement ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>, <a href="#S3.SS2.SSS1.p1" title="3.2.1 MAP-Elites extensions for QD-RL ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.1</span></a>.</span></li><li id="bib.bib44" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[35]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Y. Ng, D. Harada, and S. J. Russell</span><span class="ltx_text ltx_bib_year"> (1999)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Policy invariance under reward transformations: theory and application to reward shaping</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Sixteenth International Conference on Machine Learning</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">ICML ’99</span>, <span class="ltx_text ltx_bib_place">San Francisco, CA, USA</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;278–287</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external isbn">ISBN 1558606122</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.SSS1.p1" title="5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1.1</span></a>.</span></li><li id="bib.bib25" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[36]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">O. Nilsson and A. Cully</span><span class="ltx_text ltx_bib_year"> (2021)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Policy gradient assisted map-elites</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Genetic and Evolutionary Computation Conference</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">GECCO ’21</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;866–875</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external isbn">ISBN 9781450383509</span>, <a href="https://doi.org/10.1145/3449639.3459304" title="" class="ltx_ref ltx_bib_external">Link</a>, <a href="http://dx.doi.org/10.1145/3449639.3459304" title="" class="ltx_ref ltx_bib_external doi">Document</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p2" title="3.2.1 MAP-Elites extensions for QD-RL ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.1</span></a>, <a href="#S6.SS2.SSS4.p2" title="6.2.4 MAP-Elites and robustness ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.2.4</span></a>.</span></li><li id="bib.bib36" class="ltx_bibitem ltx_bib_misc"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">O. Nilsson</span><span class="ltx_text ltx_bib_year"> (2021)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">QDgym</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_publisher">GitHub</span>. </span><span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a href="https://github.com/ollenilsson19/QDgym" title="" class="ltx_ref ltx_font_typewriter ltx_url">https://github.com/ollenilsson19/QDgym</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p6" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S3.SS2.SSS1.p2" title="3.2.1 MAP-Elites extensions for QD-RL ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.1</span></a>, <a href="#S5.SS1.SSS1.p1" title="5.1.1 QDGym ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1.1</span></a>.</span></li><li id="bib.bib21" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[38]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Pagliuca, N. Milano, and S. Nolfi</span><span class="ltx_text ltx_bib_year"> (2020)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Efficacy of modern neuro-evolutionary strategies for continuous control optimization</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Frontiers in Robotics and AI</span> <span class="ltx_text ltx_bib_volume">7</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;98</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://www.frontiersin.org/article/10.3389/frobt.2020.00098" title="" class="ltx_ref ltx_bib_external">Link</a>, <a href="http://dx.doi.org/10.3389/frobt.2020.00098" title="" class="ltx_ref ltx_bib_external doi">Document</a>, <span class="ltx_text ltx_bib_external issn">ISSN 2296-9144</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.SSS1.p1" title="4.1.1 Approximating objective gradients with ES and actor-critic methods ‣ 4.1 Approximating Objective and Measure Gradients ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1.1</span></a>, <a href="#S5.SS2.p1" title="5.2 Experimental Design ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>, <a href="#S6.SS2.SSS5.p1" title="6.2.5 CMA-MEGA variants and gradient estimates ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.2.5</span></a>, <a href="#S6.SS2.SSS5.p3" title="6.2.5 CMA-MEGA variants and gradient estimates ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.2.5</span></a>.</span></li><li id="bib.bib33" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[39]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Pourchot and Sigaud</span><span class="ltx_text ltx_bib_year"> (2019)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CEM-RL: combining evolutionary and gradient-based methods for policy search</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Learning Representations</span>, </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://openreview.net/forum?id=BkeU5j0ctQ" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib40" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[40]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. K. Pugh, L. B. Soros, and K. O. Stanley</span><span class="ltx_text ltx_bib_year"> (2016)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Quality diversity: a new frontier for evolutionary computation</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Frontiers in Robotics and AI</span> <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;40</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://www.frontiersin.org/article/10.3389/frobt.2016.00040" title="" class="ltx_ref ltx_bib_external">Link</a>, <a href="http://dx.doi.org/10.3389/frobt.2016.00040" title="" class="ltx_ref ltx_bib_external doi">Document</a>, <span class="ltx_text ltx_bib_external issn">ISSN 2296-9144</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S5.SS1.SSS3.p1" title="5.1.3 Metrics ‣ 5.1 Evaluation Domains ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1.3</span></a>.</span></li><li id="bib.bib9" class="ltx_bibitem ltx_bib_misc"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[41]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever</span><span class="ltx_text ltx_bib_year"> (2017)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evolution strategies as a scalable alternative to reinforcement learning</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1703.03864</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S3.SS1.SSS1.p2" title="3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.1</span></a>, <a href="#S4.SS1.SSS1.p1" title="4.1.1 Approximating objective gradients with ES and actor-critic methods ‣ 4.1 Approximating Objective and Measure Gradients ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1.1</span></a>.</span></li><li id="bib.bib64" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[42]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Schaul, D. Horgan, K. Gregor, and D. Silver</span><span class="ltx_text ltx_bib_year"> (2015-07–09 Jul)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Universal value function approximators</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 32nd International Conference on Machine Learning</span>, <span class="ltx_text ltx_bib_editor">F. Bach and D. Blei (Eds.)</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">37</span>, <span class="ltx_text ltx_bib_place">Lille, France</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;1312–1320</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.mlr.press/v37/schaul15.html" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p1" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[43]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz</span><span class="ltx_text ltx_bib_year"> (2015-07–09 Jul)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Trust region policy optimization</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 32nd International Conference on Machine Learning</span>, <span class="ltx_text ltx_bib_editor">F. Bach and D. Blei (Eds.)</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">37</span>, <span class="ltx_text ltx_bib_place">Lille, France</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;1889–1897</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.mlr.press/v37/schulman15.html" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S4.SS1.SSS1.p3" title="4.1.1 Approximating objective gradients with ES and actor-critic methods ‣ 4.1 Approximating Objective and Measure Gradients ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1.1</span></a>.</span></li><li id="bib.bib14" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[44]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov</span><span class="ltx_text ltx_bib_year"> (2017)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Proximal policy optimization algorithms</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1707.06347</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1707.06347" title="" class="ltx_ref ltx_bib_external">Link</a>, <span class="ltx_text ltx_bib_external">1707.06347</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S4.SS1.SSS1.p3" title="4.1.1 Approximating objective gradients with ES and actor-critic methods ‣ 4.1 Approximating Objective and Measure Gradients ‣ 4 Approximating Gradients for CMA-MEGA ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1.1</span></a>, <a href="#S6.SS2.SSS5.p1" title="6.2.5 CMA-MEGA variants and gradient estimates ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.2.5</span></a>.</span></li><li id="bib.bib1" class="ltx_bibitem ltx_bib_book"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[45]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. S. Sutton and A. G. Barto</span><span class="ltx_text ltx_bib_year"> (2018)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Reinforcement learning: an introduction</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_edition">Second edition</span>, <span class="ltx_text ltx_bib_publisher">The MIT Press</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://incompleteideas.net/book/the-book-2nd.html" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Quality Diversity for Reinforcement Learning (QD-RL) ‣ 2 Problem Statement ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.</span></li><li id="bib.bib47" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[46]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Tang</span><span class="ltx_text ltx_bib_year"> (2021)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Guiding evolutionary strategies with off-policy actor-critic</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">AAMAS ’21</span>, <span class="ltx_text ltx_bib_place">Richland, SC</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;1317–1325</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external isbn">ISBN 9781450383073</span></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="3.3 Diversity in Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.</span></li><li id="bib.bib35" class="ltx_bibitem ltx_bib_misc"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[47]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Tjanaka, M. C. Fontaine, Y. Zhang, S. Sommerer, N. Dennler, and S. Nikolaidis</span><span class="ltx_text ltx_bib_year"> (2021)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pyribs: a bare-bones python library for quality diversity optimization</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_publisher">GitHub</span>. </span><span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a href="https://github.com/icaros-usc/pyribs" title="" class="ltx_ref ltx_font_typewriter ltx_url">https://github.com/icaros-usc/pyribs</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS3.p1" title="5.3 Implementation ‣ 5 Experiments ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.3</span></a>.</span></li><li id="bib.bib26" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[48]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Vassiliades and J. Mouret</span><span class="ltx_text ltx_bib_year"> (2018)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Discovering the elite hypervolume by leveraging interspecies correlation</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Genetic and Evolutionary Computation Conference</span>, </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">GECCO ’18</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;149–156</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external isbn">ISBN 9781450356183</span>, <a href="https://doi.org/10.1145/3205455.3205602" title="" class="ltx_ref ltx_bib_external">Link</a>, <a href="http://dx.doi.org/10.1145/3205455.3205602" title="" class="ltx_ref ltx_bib_external doi">Document</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p2" title="3.2.1 MAP-Elites extensions for QD-RL ‣ 3.2 Quality Diversity Algorithms ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2.1</span></a>, <a href="#S6.SS2.SSS4.p2" title="6.2.4 MAP-Elites and robustness ‣ 6.2 Discussion ‣ 6 Results ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.2.4</span></a>.</span></li><li id="bib.bib20" class="ltx_bibitem ltx_bib_article"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[49]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, and J. Schmidhuber</span><span class="ltx_text ltx_bib_year"> (2014)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Natural evolution strategies</span>. </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Machine Learning Research</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">27</span>), <span class="ltx_text ltx_bib_pages">pp.&nbsp;949–980</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://jmlr.org/papers/v15/wierstra14a.html" title="" class="ltx_ref ltx_bib_external">Link</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>, <a href="#S3.SS1.SSS1.p1" title="3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.1</span></a>, <a href="#S3.SS1.SSS1.p2" title="3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.1</span></a>.</span></li><li id="bib.bib60" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[50]</span> <span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Wierstra, T. Schaul, J. Peters, and J. Schmidhuber</span><span class="ltx_text ltx_bib_year"> (2008)</span> </span><span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Natural evolution strategies</span>. </span><span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)</span>, </span><span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume"></span>, <span class="ltx_text ltx_bib_pages">pp.&nbsp;3381–3387</span>. </span><span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1109/CEC.2008.4631255" title="" class="ltx_ref ltx_bib_external doi">Document</a></span> </span><span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.SSS1.p1" title="3.1.1 Evolution strategies (ES) ‣ 3.1 Single-Objective Reinforcement Learning ‣ 3 Background ‣ Approximating Gradients for Differentiable Quality Diversity in Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1.1</span></a>.</span></li></ul></section></article></div></main><div class="__preact_toc__"></div><script src="/assets/main.js?380004d8e6cf96e7b73a6e752478eca0"></script><script src="/assets/vendor.js?786513472d404941bcd6e14f6893c8cf"></script><script src="/assets/blazy.min.js"></script><script>;(function(){const bLazy = new Blazy();})();</script></body></html>